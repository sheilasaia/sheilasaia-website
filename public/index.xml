<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>  on  </title>
    <link>/</link>
    <description>Recent content in   on  </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Let&#39;s Talk About Water: Lost Waterways of Winston-Salem</title>
      <link>/talk/ltaw_sept2019/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 -0400</pubDate>
      
      <guid>/talk/ltaw_sept2019/</guid>
      <description>

&lt;h2 id=&#34;abstract-br&#34;&gt;Abstract:&lt;/br&gt;&lt;/h2&gt;

&lt;p&gt;The Lost Waterways of Winston-Salem is a free film screening and panel discussion at &lt;a href=&#34;https://www.aperturecinema.com/f-aq/&#34; target=&#34;_blank&#34;&gt;a/perture cinema&lt;/a&gt;. Drawing on the Winston-Salem motto of the &amp;ldquo;City of Arts and Innovation,&amp;rdquo; this event seeks to engage students and community members in a conversation about local water issues by highlighting the connection between STEM and the arts. Beginning at 5:00 PM, all are invited to the Chatham Building Atrium (305 W. 4th St.) to enjoy refreshments, engage with community partners, and explore environmentally-themed artwork by local artists. The screening of &lt;a href=&#34;https://www.imdb.com/title/tt2421498/&#34; target=&#34;_blank&#34;&gt;Lost Rivers (2012)&lt;/a&gt; will begin at 6pm in a/perture studios 1 &amp;amp; 2. Immediately following the screening, expert panelists will engage in a discussion moderated by Linda Lilienfeld, the &lt;a href=&#34;http://letstalkaboutwater.com/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Let&amp;rsquo;s Talk about Water&amp;rdquo;&lt;/a&gt; Founder and Project Coordinator. Panelists include: Dr. Lauren Lowman, Dr. Sheila Saia, Kristen Ford Haaf, and Christine Rucker.&lt;/p&gt;

&lt;p&gt;To read more and RSVP for this free event, visit &lt;a href=&#34;https://www.eventbrite.com/e/lost-waterways-of-winston-salem-film-screening-panel-discussion-tickets-65981950791&#34; target=&#34;_blank&#34;&gt;this website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/lost_waterways_2019.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Intro to Preprints for Early Career Hydrologists</title>
      <link>/post/2019-05-19-preprint-intro/</link>
      <pubDate>Mon, 20 May 2019 21:12:00 -0500</pubDate>
      
      <guid>/post/2019-05-19-preprint-intro/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;When I attended the 2019 &lt;a href=&#34;https://sites.agu.org/&#34;&gt;American Geophysical Union (AGU)&lt;/a&gt; this past December in Washington, DC, I had the pleasure of meeting members of an international organization of early career hydrologists called the &lt;a href=&#34;https://younghs.com/&#34;&gt;Young Hydrologic Society (YHS)&lt;/a&gt;. To clarify, “young” here refers to early career, rather than age. I was interested in getting more involved in the organization and thought writing a blog post for the group’s website would be a great way for me to contribute. I had recently lead a discussion on preprints with peers back in North Carolina and thought a blog post on this topic would be a perfect opportunity to continue that discussion. I also had (and still have) a lot of questions regarding preprints and hoped that preparing the blog post would give the opportunity to answer these questions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of This Post&lt;/h1&gt;
&lt;p&gt;The blog post that follows is re-posted of the YHS article I wrote on preprints. You can find the original article &lt;a href=&#34;https://younghs.com/2019/05/20/an-introduction-to-preprints-for-early-career-hydrologists/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks to H. Beria, C. Hall, S. Harrigan, C. Jackson, and N. Krell for their helpful feedback on this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Growing calls for open and reproducible research across science, technology, engineering, and math (STEM) disciplines have advanced the conversation around preprints (e.g., Schloss, 2017; Narock et al., 2019). Early Career Hydrologists may benefit from considering and discussing the role of preprints in shaping scientific discovery and career trajectories. Here we introduce preprints, offering Early Career Hydrologists with a variety of thoughts on the advantages and disadvantages of using preprints in research workflows, and providing tips and resources for learning more. If we missed an aspect of the preprint discussion that you feel passionate about or still have questions about, please feel free to reach out to Sheila (at &lt;a href=&#34;https://twitter.com/sheilasaia?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor&#34;&gt;sheilasaia&lt;/a&gt;) and the Young Hydrology Society (YHS; at &lt;a href=&#34;https://twitter.com/younghydrology?lang=en&#34;&gt;YoungHydrology&lt;/a&gt;) on Twitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-preprint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a preprint?&lt;/h1&gt;
&lt;p&gt;A preprint refers to a research product (typically a research article) that is made publicly available before or at the same time it goes to peer review. A preprint server refers to an open access website where authors can submit and manage versions of their preprints.&lt;/p&gt;
&lt;p&gt;Some preprint servers such as European Geophysical Union (EGU) sponsored &lt;a href=&#34;https://www.hydrol-earth-syst-sci-discuss.net/discussion_papers.html&#34;&gt;Hydrology and Earth Systems Science (HESS) Discussions&lt;/a&gt; is affiliated with EGU’s HESS journal. Additionally, the American Geophysical Union (AGU) sponsored Earth and Space Science Open Archive (ESSOAr) preprint server is associated with AGU-affiliated journals. These journal-supported preprint servers offer a convenient publishing pipeline should the author’s work be accepted after peer review.&lt;/p&gt;
&lt;p&gt;Other preprint servers are not affiliated with a particular journal (e.g., &lt;a href=&#34;https://eartharxiv.org/&#34;&gt;EarthArXiv&lt;/a&gt;, and &lt;a href=&#34;https://www.biorxiv.org/&#34;&gt;bioRxiv&lt;/a&gt;. If an authors’ article is accepted for publication to a partner journal, these servers may offer transfer services for authors. For example, the bioRxiv offers transfer services to several journals including Applied and Environmental Microbiology (see details &lt;a href=&#34;https://www.biorxiv.org/about-biorxiv&#34;&gt;here&lt;/a&gt;). Preprint servers are typically not-for-profit (e.g., arXiv, bioRxiv, EarthArXiv); it is always good to check a preprint server’s not-for-profit status before submitting a preprint. See the ‘Things to Consider before Sharing a Preprint’ discussion below for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advantages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Advantages:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Depending on the preprints server, authors may be given a digital object identifier (DOI) number that can be used to cite their work. Also, the preprint server manages the storage and versions of the preprint free of charge. The DOI allows others to cite, and ultimately, give authors credit for their ideas and work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The preprint (and eventually a postprint, if accepted by a journal) can be shared more broadly. This is especially true of the final publication, since a journal subscription is not required.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Preprints improve the transparency of the peer review process by enabling readers to view article versions at each step of the review process; and thus, assess changes they’ve made.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Preprint servers enable discussions between authors and interested parties. Readers can contribute to and look through these discussions via preprint server commenting features and direct communication with authors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Posting a preprint allows author share so-called ‘negative results’, which may improve future studies and conserving future time and funding resources (van Emmerik et al. 2018)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Preprints may help streamline collaborations by avoiding or improving studies in different locations that address similar research goals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Preprint servers empower scientists to take control of the timeline for making their results public (Schloss 2017). This might prove especially helpful to overcoming current limitations of peer review (e.g., gender bias; Grogan 2019).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Preprints can help facilitate research findings to the public domain, so as to stimulate interactions and collaborations with fellow researchers. This may be especially helpful for Earth Career Scientists looking for potential collaborators inside and outside of their field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Earth Career Scientists may be able to use preprints in grant and job applications to present progress and preliminary results. For example, the US National Institutes of Health allows scientists to include preprints in grant proposals (Kaiser 2017a, 2017b).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some disciplines (e.g., physics) use preprint servers to stake claim to their results (or at least have a DOI associated with their early work).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;disadvantages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disadvantages:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The discussion of preprint disadvantages largely centers around the fact that preprints are not validated by peer review (Kaiser 2017b). Researchers can make claims, that may be cited by others, without valid data supporting those claims. This may become especially problematic at the interface of science and the general public, especially if the general public and media representatives do not understand that the preprint is not peer reviewed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another common criticism of preprints centers around fear of being “scooped”—when an author’s work is stolen before it can be published. [On the other hand, authors of preprints have a DOI. Also, some journals have announced “scooping protection” (Kaiser 2017b).]&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Researchers that post preprints need to do their homework before making their work public. See the ‘Things to Consider before Sharing a Preprint’ discussion below. Most notably, it is important that authors know what their journal-of-choice’s preprint policies are in terms of accepting preprints, type of preprint licenses, and preprint server funding structures (i.e., non-for-profit) that are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;things-to-consider-before-sharing-a-preprint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Things to Consider Before Sharing a Preprint&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Check whether the journal you are planning to submit your work to accepts preprints. You can check a journal’s preprint policy on &lt;a href=&#34;http://www.sherpa.ac.uk/romeo/index.php&#34;&gt;SHERPA/RoMEO&lt;/a&gt;. Many journals accept preprints posted on not-for-profit preprint servers, but it is best to confirm that this is the case. When in doubt, you can email the editor to clarify the journal’s preprint policy. In addition to the funding structure of the preprint server (e.g., not-for-profit), be sure to pay attention to the journal’s policies related to licenses that are assigned by the preprint servers (e.g., CC BY).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ask your co-authors for permission to post your article as a preprint, which includes asking them about their institution’s or company’s preprint policy. Before you post, you want to make sure everyone agrees to post your article as a preprint.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure your article is ready to be posted as a preprint. Once your preprint is in the public domain, it will be very hard to remove.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;where-can-i-post-andor-read-a-hydrology-preprint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Where can I post and/or read a hydrology preprint?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/&#34;&gt;arXiv&lt;/a&gt; is the oldest preprint server to-date (since 1991) and hosts research from physics, mathematics, computer science, statistics, engineering, and various other disciplines that interface with the hydrologic sciences. As of May 2019 (when this post was written), there were 241 arXiv search results for “hydrology” (0.02% of the entire collection).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inspired by the arXiv, researchers established the &lt;a href=&#34;https://eartharxiv.org/&#34;&gt;EarthArXiv&lt;/a&gt; in 2017 to serve the earth sciences community; thus, hydrology is particularly well represented here. EarthArXiv is part of the &lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework (OSF)&lt;/a&gt; initiative.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Last but not least, the &lt;a href=&#34;https://www.essoar.org/&#34;&gt;Earth and Space Science Open Archive (ESSOAr)&lt;/a&gt; preprint server was established in 2018 through a joint effort with the American Geophysical Union (AGU) and &lt;a href=&#34;https://www.atypon.com/&#34;&gt;Atypon&lt;/a&gt;. ESSOAr is especially well suited for research that will be submitted to AGU-affiliated journals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;can-and-how-do-i-cite-a-preprint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can (and how do) I cite a preprint?&lt;/h1&gt;
&lt;p&gt;As mentioned above, preprints need to be treated as unpublished, non-peer reviewed publications. As an author and reader, you need to assess the validity and quality of the work before you cite a preprint. Before submitting an article that cites preprinted work, you should check that the journal allows preprint citations. If the journal allows preprints, authors should adhere to the formatting guidelines to make it clear to reviewers/readers that the work is not peer-reviewed. The same goes for funding agencies. An example policy on how to cite preprints can be found for Nature-affiliated journals at &lt;a href=&#34;https://www.nature.com/authors/policies/preprints.html&#34; class=&#34;uri&#34;&gt;https://www.nature.com/authors/policies/preprints.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Grogan, K. 2019. How the entire scientific community can confront gender bias in the workplace. Nature Ecology &amp;amp; Evolution. 3:3-6. Accessed online at: &lt;a href=&#34;https://www.nature.com/articles/s41559-018-0747-4&#34; class=&#34;uri&#34;&gt;https://www.nature.com/articles/s41559-018-0747-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaiser, J. 2017a. NIH enables investigators to include draft preprints in grant proposals. Science. Mar. 24, 2017. Accessed online at: &lt;a href=&#34;https://www.sciencemag.org/news/2017/03/nih-enables-investigators-include-draft-preprints-grant-proposals&#34; class=&#34;uri&#34;&gt;https://www.sciencemag.org/news/2017/03/nih-enables-investigators-include-draft-preprints-grant-proposals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaiser, J. 2017b. The preprint dilemma. Science. 357(6358):1344-1349. Available online at: &lt;a href=&#34;https://science.sciencemag.org/content/357/6358/1344&#34; class=&#34;uri&#34;&gt;https://science.sciencemag.org/content/357/6358/1344&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Narock, T., E. Goldstein, C. Jackson, A. Bubeck, A. Enright, J. Farquharson, et al. Earth Science is Ready for Preprints. Eos. Apr. 23, 2019. Available online at: &lt;a href=&#34;https://eartharxiv.org/kftsv/&#34; class=&#34;uri&#34;&gt;https://eartharxiv.org/kftsv/&lt;/a&gt; and &lt;a href=&#34;https://eos.org/project-updates/earth-science-is-ready-for-preprints&#34; class=&#34;uri&#34;&gt;https://eos.org/project-updates/earth-science-is-ready-for-preprints&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schloss, P. D. 2017. Preprinting Microbiology. mbio. 8:e00438-17. Available online at: &lt;a href=&#34;https://mbio.asm.org/content/8/3/e00438-17&#34; class=&#34;uri&#34;&gt;https://mbio.asm.org/content/8/3/e00438-17&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van Emmerik, T., A. Popp, A. Solcerova, H. Muller, and R. Hut. 2018. Reporting negative results to stimulate experimental hydrology: discussion of “The role of experimental work in hydrological sciences - insights from a community survey”. Hydrological Sciences Journal. 63:1269-1273. &lt;a href=&#34;https://doi.org/10.1080/02626667.2018.1493203&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/02626667.2018.1493203&lt;/a&gt;. Available online at: &lt;a href=&#34;https://eartharxiv.org/jhrfb/&#34; class=&#34;uri&#34;&gt;https://eartharxiv.org/jhrfb/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-reading-on-preprints&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Additional Reading on Preprints&lt;/h1&gt;
&lt;p&gt;Nature Geosciences (editorial). 2018. ArXives of Earth Science. Nature Geosciences. 11: 149. Available online at: &lt;a href=&#34;https://www.nature.com/articles/s41561-018-0083-y&#34; class=&#34;uri&#34;&gt;https://www.nature.com/articles/s41561-018-0083-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ESSOAr FAQs: &lt;a href=&#34;https://www.essoar.org/faq&#34; class=&#34;uri&#34;&gt;https://www.essoar.org/faq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PLOS Preprint Resources: &lt;a href=&#34;https://www.plos.org/preprints&#34; class=&#34;uri&#34;&gt;https://www.plos.org/preprints&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Additional Thoughts&lt;/h1&gt;
&lt;p&gt;This post is meant to provide introductory information to early career hydrologists interested in sharing their work publicly via preprint. At this time, I’ve only submitted one paper for preprint; therefore, I’m inexperienced. It’d be interesting to hear from other early career scientists about how they’ve negotiated publication of preprints with more established scientists, particularly when it addressing points listed under ‘Disadvantages’ above.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Calculating the Cost of My Flight&#39;s Carbon Footprint in R</title>
      <link>/post/2019-04-19-carbon-cost-calcs/</link>
      <pubDate>Thu, 18 Apr 2019 21:12:00 -0500</pubDate>
      
      <guid>/post/2019-04-19-carbon-cost-calcs/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Much of my postdoctoral research is focused on studying (and building tools to help decision makers mitigate) the impacts of climate change on communities in North Carolina. I’m also committed to reducing my carbon footprint and have read &lt;a href=&#34;https://www.nytimes.com/2017/07/27/climate/airplane-pollution-global-warming.html&#34;&gt;over&lt;/a&gt; and &lt;a href=&#34;https://www.vox.com/energy-and-environment/2019/1/11/18177118/airlines-climate-change-emissions-travel&#34;&gt;over&lt;/a&gt; that flying makes up the largest fraction of an average person’s carbon dioxide emissions. In early January I decided that I wanted to try to reduce the number of flights I took in 2019. I also wanted to figure out how to offset the cost of my carbon dioxide emissions when it was going to be tough to avoid flying.&lt;/p&gt;
&lt;p&gt;I started researching websites to help me figure out how much to offset. I found several tools to calculate carbon dioxide emissions (e.g., &lt;a href=&#34;https://www.carbonfootprint.com/calculator.aspx&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.carbonify.com/carbon-calculator.htm&#34;&gt;here&lt;/a&gt;) but couldn’t find any that told me how much to donate based on these emissions. Early on, I decided my offsets would go to a local organization addressing climate change. For me this looked like a non-profit focused on building solar panels in the Raleigh-Durham area, but for you, it might be something different. I was doing these calculations manually and then I realized it would be more fun and more efficient to automate them in &lt;code&gt;R&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;In this post, I’m starting with flights but maybe I’ll branch out to &lt;a href=&#34;https://www.yaleclimateconnections.org/2015/09/evolving-climate-math-of-flying-vs-driving/&#34;&gt;driving&lt;/a&gt; in the future…and maybe someday a shiny app for both.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE!&lt;/strong&gt; If you’re not an R-user but you’d still like to learn how much you should donate to offset the carbon dioxide you emit from flying, &lt;em&gt;please scroll to the bottom of this post&lt;/em&gt;. I’ve included step-by-step directions for how to do this without &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of This Post&lt;/h1&gt;
&lt;p&gt;The main goals of this post are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Calculate how much carbon dioxide is emitted for flights&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate how much to donate based on this carbon footprint&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a function to automate these calculations&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks to friends for their encouragement and to the &lt;a href=&#34;https://www.fridaysforfuture.org/&#34;&gt;#fridaysforthefuture&lt;/a&gt; movement for inspiring this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set Up&lt;/h1&gt;
&lt;p&gt;Let’s load the library we’ll need for this post. We’ll use the &lt;a href=&#34;https://cran.r-project.org/web/packages/airportr/vignettes/Introduction_to_Airportr.html&#34;&gt;&lt;code&gt;airportr&lt;/code&gt; package&lt;/a&gt;, which has helpful functions for looking up airport codes and calculating the distances between airports.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(airportr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;all-the-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;All the Airports&lt;/h1&gt;
&lt;p&gt;There are many airports around the world and you might not know the 3-letter International Air Transport Association (IATA) code for the airports. Luckily, the &lt;a href=&#34;https://cran.r-project.org/web/packages/airportr/vignettes/Introduction_to_Airportr.html&#34;&gt;&lt;code&gt;airportr&lt;/code&gt; package&lt;/a&gt; has a look-up table function to help with this.&lt;/p&gt;
&lt;p&gt;In this example, let’s calculate the amount we can donate to offset the carbon dioxide emissions for a flight from Raleigh/Durham, NC (departure city) to Charlotte, NC (arrival city).&lt;/p&gt;
&lt;p&gt;Let’s start by looking up airports with “Raleigh” in their name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airport_lookup(input = &amp;quot;Raleigh&amp;quot;, input_type = &amp;quot;name&amp;quot;, output_type = &amp;quot;IATA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in airport_lookup(input = &amp;quot;Raleigh&amp;quot;, input_type = &amp;quot;name&amp;quot;,
## output_type = &amp;quot;IATA&amp;quot;): No exact matches but some similar names in the
## database include:&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## General Ignacio P. Garcia International Airport
## Licenciado y General Ignacio Lopez Rayon Airport
## Rabigh Airport
## Raleigh Durham International Airport
## Raleigh County Memorial Airport
## Leigh Creek Airport&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There were several results but the Raleigh Durham International Airport is the one we want. Let’s use that as the new look-up function input.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airport_lookup(input = &amp;quot;Raleigh Durham International Airport&amp;quot;, input_type = &amp;quot;name&amp;quot;, output_type = &amp;quot;IATA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;RDU&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time we get get the 3-digit IATA code for our departure city: RDU.&lt;/p&gt;
&lt;p&gt;If you think you know the 3-digit IATA code for your airport of interests, you can also use the look-up functions to double check. Let’s do this for the arrival city, which I’m pretty sure is CLT.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airport_lookup(input = &amp;quot;CLT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Charlotte Douglas International Airport&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! Looks like CLT was a good guess for for the Charlotte, NC airport.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-the-distance-between-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculate the Distance Between Airports&lt;/h1&gt;
&lt;p&gt;Now that we know the departure and arrival city IATA codes, we can use them to calculate the distance between RDU and CLT. The &lt;code&gt;airportr&lt;/code&gt; package also has a function for this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kilometers &amp;lt;- round(airport_distance(&amp;quot;RDU&amp;quot;, &amp;quot;CLT&amp;quot;))
miles &amp;lt;- round(airport_distance(&amp;quot;RDU&amp;quot;, &amp;quot;CLT&amp;quot;) * 0.621)

kilometers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 209&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;miles&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 130&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 209 km (or 130 mi) between the RDU and CLT airports (as a crow flies).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;carbon-dioxide-emission-calculations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Carbon Dioxide Emission Calculations&lt;/h1&gt;
&lt;p&gt;To calculate our carbon dioxide emissions we’ll need to know how much carbon dioxide is emitted per mile traveled per person. Thankfully, many people before me have worked out this calculation. One of the most thorough descriptions of this is available at &lt;a href=&#34;https://blueskymodel.org/air-mile&#34;&gt;blueskymodel.org&lt;/a&gt;. After stepping through their calculations, this website recommends that 1 air mile produces 0.24 pounds of carbon dioxide emitted per person.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Radiative_forcing&#34;&gt;Radiative forcing&lt;/a&gt; is important to consider with respect to flight emissions calculations because it allows us to account for the fact that our plane is closer to the top of the atmosphere when it is admitting carbon dioxide. This matters because carbon dioxide emitted higher up in the Earth’s atmosphere has a greater warming potential then carbon dioxide emitted by our cars, etc. on the Earth’s surface. To account for radiative forcing, we can multiply our emission calculation by a factor. The website &lt;a href=&#34;https://carbonfund.org/how-we-calculate/&#34;&gt;carbonfund.org&lt;/a&gt; recommends this factor be equal to 1.891.&lt;/p&gt;
&lt;p&gt;The full calculation for metric tons of carbon oxide emitted with radiative forcing is given below. The (1/2204.62) helps us convert from pounds to metric tons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num_people &amp;lt;- 1
co2_emitted &amp;lt;- miles * num_people * 0.24 * (1 / 2204.62) * 1.891

round(co2_emitted, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When one person travels one-way from RDU to CLT, they emit 0.027 metric tons (or 59 pounds) of carbon dioxide.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;carbon-dioxcide-offset-calculations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Carbon Dioxcide Offset Calculations&lt;/h1&gt;
&lt;p&gt;Next, we’ll need to figure out how much to donate given these carbon dioxide emissions. To do this, we can refer to the US Environmental Protection Agency’s (USEPA) &lt;a href=&#34;https://19january2017snapshot.epa.gov/sites/production/files/2016-12/documents/sc_co2_tsd_august_2016.pdf&#34;&gt;Social Cost of Carbon Technical Report&lt;/a&gt;. The information we’ll need is found in Table 2 of this document. We can save this information to a dataframe. The social cost of carbon includes the impact of carbon dioxide emissions on: “…changes in net agricultural productivity, human health, property damages from increased flood risk, and changes in energy system costs, such as reduced costs for heating and increased costs for air conditioning.” (&lt;a href=&#34;https://19january2017snapshot.epa.gov/climatechange/social-cost-carbon_.html&#34;&gt;USEPA, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;social_cost_co2 &amp;lt;- data.frame(
  year = seq(2015, 2050, 5),
  avg_5_perc_usd_per_ton_co2 = c(11, 12, 14, 16, 18, 21, 23, 26),
  avg_3_perc_usd_per_ton_co2 = c(36, 42, 46, 50, 55, 60, 64, 69),
  avg_2.5_perc_usd_per_ton_co2 = c(56, 62, 68, 73, 78, 84, 89, 95),
  high_impact_usd_per_ton_co2 = c(105, 123, 138, 152, 168, 183, 197, 212)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s walk through each of these columns. Column 1 is the year, which ranges from 2015 to 2050. Columns 2 to 5 all represent the cost (in US dollars) of emitting a metric ton of carbon dioxide given different future strategies. That is, the average (of three climate models times five climate change scenarios) cost given a 5%, 3%, and 2.5% discount rate for columns 2 through 4, respectively. I’ll admit that I don’t fully understand the discount rate, but believe it’s used as a way to consider whether people are more likely to value short-term or long-term risks associated with climate change. The larger the discount rate, the less they value long-term risk. In a societal context, the larger discount rate suggests that current generations are valued more than future generations. The report also includes a high impact case that is calculated from the 95th percentile (of three climate models times five climate change scenarios), rather than the average, cost given a 3% discount rate. The USEPA’s &lt;a href=&#34;https://19january2017snapshot.epa.gov/sites/production/files/2016-12/documents/sc_co2_tsd_august_2016.pdf&#34;&gt;Social Cost of Carbon Technical Report&lt;/a&gt; report says, “…there is extensive evidence in the scientific and economic literature of the potential for lower-probability, higher-impact outcomes from climate change, which would be particularly harmful to society and thus relevant to the public and policymakers.” Therefore, for this blog post, we’ll move forward using the high impact case.&lt;/p&gt;
&lt;p&gt;We can fit this linear trend and use it to estimate the cost of a metric ton of carbon dioxide emitted for years not in Table 2 (i.e., for this year, 2019).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit a linear model
cost_lm &amp;lt;- lm(high_impact_usd_per_ton_co2 ~ year, data = social_cost_co2)

# save the model parameters for prediction
intercept &amp;lt;- cost_lm$coefficients[1]
slope &amp;lt;- cost_lm$coefficients[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the data (points) and linear model (line).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(high_impact_usd_per_ton_co2 ~ year, data = social_cost_co2, pch = 16, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;Cost per Metric Ton CO2 Emitted (USD)&amp;quot;)
abline(a = intercept, b = slope, col = &amp;quot;red&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-19-carbon-cost-calcs_files/figure-html/plot%20cost%20model-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, now we can use the model parameters to predict the cost for a given year. Let’s try this for 2019.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_year &amp;lt;- 2019
my_year * slope + intercept&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     year 
## 118.9286&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The social cost of a metric ton of carbon dioxide emitted in 2019 is $119.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together-aka-making-an-r-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting it All Together (aka Making an R Function)&lt;/h1&gt;
&lt;p&gt;Now let’s take all the code we wrote above and combine it into a custom R function that calculates how much we can donate to offset the carbon dioxide emissions for a new flight. In this second example, we’ll fly round-trip from Raleigh/Durham, NC (departure city) to San Francisco, CA (arrival city).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon_offset_cost &amp;lt;- function(departing_airport_code, arriving_airport_code, year, num_people, radiative_force = TRUE, round_trip = FALSE) {
  # year should range from 2015 - 2050 to assume linearity
  # departing_airport_code and arriving_airport code are three letter airport codes
  # radiative_force = TRUE (default) will use radiative forcing in the calculation
  # set to FALSE to avoid using radiative forcing
  # round_trip = FALSE (default) will double the cost calculation if set to TRUE
  # carbon offset result is given in USD
  
  # note: you will need to load the airportr package before using this function
  
  # linear coefficients from high impact column of table 2
  # source: https://19january2017snapshot.epa.gov/sites/production/files/2016-12/documents/sc_co2_tsd_august_2016.pdf
  intercept &amp;lt;- (-5986.143)
  slope &amp;lt;- 3.02381
  
  # calculate miles flown
  miles &amp;lt;- round(airport_distance(departing_airport_code, arriving_airport_code) * 0.621)
  
  if (radiative_force == TRUE) {
    # metric tons of co2 emitted w/ radiative forcing
    co2_emitted &amp;lt;- miles * num_people * 0.24 * (1 / 2204.62) * 1.891
    # source: https://blueskymodel.org/air-mile (for emission conversion factor per person)
    # source: https://carbonfund.org/how-we-calculate/ (for carbon forcing coefficient)
    
    # cost in usd
    if (round_trip == FALSE) {
      # one-way
      cost &amp;lt;- co2_emitted * (slope * year + intercept)
    }
    
    else if (round_trip == TRUE) {
      # round-trip
      cost &amp;lt;- co2_emitted * (slope * year + intercept) * 2
    }
  }
  
  else if (radiative_force == FALSE) {
    # metric tons of co2 emitted w/out radiative forcing
    co2_emitted &amp;lt;- miles * num_people * 0.24 * (1 / 2204.62) 
    
    # cost in usd
    if (round_trip == FALSE) {
      # one-way
      cost &amp;lt;- co2_emitted * (slope * year + intercept)
    }
    
    else if (round_trip == TRUE) {
      # round-trip
      cost &amp;lt;- co2_emitted * (slope * year + intercept) * 2
    }  
  }
  
  else {
    cost &amp;lt;- NA
  }
  
  return(cost)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s test out the function for a round-trip flight from RDU to SFO (for 1 person).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon_offset_cost(departing_airport_code = &amp;quot;RDU&amp;quot;, 
                   arriving_airport_code = &amp;quot;SFO&amp;quot;,
                   year = 2019,
                   num_people = 1,
                   radiative_force = TRUE,
                   round_trip = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 117.2229&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like we can donate $117 to ofset the carbon dioxide emitted from our round-trip flight from RDU to SFO in 2019. If we want to be more generous we can adjust the year for a future period. For example, if we change the year to the maximum value, 2050, we can donate $210. Our emissions are more expensive the further out in time we go (see the &lt;a href=&#34;https://19january2017snapshot.epa.gov/climatechange/social-cost-carbon_.html&#34;&gt;USEPA Social Cost of Carbon website&lt;/a&gt; for more information).&lt;/p&gt;
&lt;p&gt;Some other thoughts that I wanted to mention before signing off:&lt;/p&gt;
&lt;p&gt;Unless you’re an R user, I recognize that it will take extra work for you to figure out how much you should donate to offset your flight’s carbon dioxide emissions. The first task is for me to start writing a second post for non-R users, and in the mean time, I’ll give an example below to put this all into perspective for &lt;em&gt;non-R users&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-im-not-an-r-user&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What if I’m not an R user?&lt;/h1&gt;
&lt;p&gt;Here are some of the basic calculations you’d need to make if you are not an R user and want to calculate your carbon foot print for a flight you’re taking.&lt;/p&gt;
&lt;p&gt;Steps:&lt;br/&gt; 1. Determine the mileage between your arrival and destination airports in miles (i.e., jot this down as variable &lt;code&gt;miles&lt;/code&gt;). Use a website like &lt;a href=&#34;https://www.airmilescalculator.com/&#34;&gt;this&lt;/a&gt;.&lt;br/&gt; 2. Decide how many people (i.e., jot this down as variable &lt;code&gt;num_people&lt;/code&gt;) are traveling and whether the trip is a round-trip or one way.&lt;br/&gt; 3. If a one-way trip, use the formula: &lt;code&gt;co2_emitted = miles x num_people x 0.24 x (1 / 2204.62) x 1.891&lt;/code&gt; to determine the metric tons of carbon dioxide emitted from your flight. Don’t forget the &lt;a href=&#34;https://www.mathsisfun.com/operation-order-pemdas.html&#34;&gt;order of operations&lt;/a&gt; here. ;) The variable &lt;code&gt;miles&lt;/code&gt; comes from step 1 and the variable &lt;code&gt;num_people&lt;/code&gt; comes from step 2. If round-trip, calculate &lt;code&gt;co2_emitted&lt;/code&gt; as mentioned previously and multiply this by 2 since you’re traveling to your destination and back.&lt;br/&gt; 4. Think about whether you view climate change as a short- or long-term goal. If you think of it based on short-term impacts only then choose the current year (i.e., jot this down as variable &lt;code&gt;year&lt;/code&gt; = 2019). However, if you are commited to addressing its more long-term impacts, you can choose any year up until 2050 (i.e. jot this down as variable &lt;code&gt;year&lt;/code&gt; = 2050). The closer to 2050 you choose, the larger your donation will be.&lt;br/&gt; 5. Calculate the cost of your offset (i.e., how much you’ll donate) using the formula: &lt;code&gt;cost = co2_emitted x (3.02 x year - 5986.1)&lt;/code&gt;. Remember your &lt;a href=&#34;https://www.mathsisfun.com/operation-order-pemdas.html&#34;&gt;order of operations&lt;/a&gt;. The variable &lt;code&gt;co2_emitted&lt;/code&gt; comes from step 3 and the variable &lt;code&gt;year&lt;/code&gt; comes from step 4.&lt;br/&gt; 6. Donate this amount (in US dollars) to your local climate change organization (e.g., one building solar panels, planting trees, or advocating for bicycle lanes).&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If have questions, comments, or ideas related to this post, please let me know!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applying Climate Change Risk Management Tools to Integrate Streamflow Projections and Social Vulnerability</title>
      <link>/publication/yadkin-swat-svi/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 -0400</pubDate>
      
      <guid>/publication/yadkin-swat-svi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R Resources for Water Scientists</title>
      <link>/post/2019-02-15-water-resources/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-02-15-water-resources/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;There are lots of resources out there for R users and I thought it might be helpful to compile some that might be especially helpful for &lt;strong&gt;water scientists&lt;/strong&gt;. I’ll also admit that this is a 50% selfish task meant to help me keep track of resources for my own learning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of This Post&lt;/h1&gt;
&lt;p&gt;The main goal of this blog post is to compile links to &lt;code&gt;R&lt;/code&gt; resources including: packages, tutorials, books, etc. that might be of interests to water scientists.&lt;/p&gt;
&lt;p&gt;Note to self that someday maybe I can update this post to also include &lt;code&gt;Python&lt;/code&gt; resources. For now, I’m sticking to &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks to all of the people who’ve worked hard to share their code and insights together to benefit us all. I love the open source programming community!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Packages&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Water-related packages currated by &lt;a href=&#34;https://samzipper.weebly.com/&#34;&gt;Sam Zipper&lt;/a&gt; at this Google Sheet document &lt;a href=&#34;https://docs.google.com/document/d/1XM3KuWdMgVrh92zE-CGIFNC7jYd_DGD8CokAVLkItz0/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CRAN tasks views for R packages related to &lt;a href=&#34;https://cran.r-project.org/web/views/Environmetrics.html&#34;&gt;envirometrics&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/views/Hydrology.html&#34;&gt;hyrdrology&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/views/Spatial.html&#34;&gt;spatial analysis&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/views/TimeSeries.html&#34;&gt;time series analysis&lt;/a&gt;, and &lt;a href=&#34;https://cran.r-project.org/web/views/&#34;&gt;a number of other topics&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tutorials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vignettes for the &lt;code&gt;sf&lt;/code&gt; package &lt;a href=&#34;https://github.com/r-spatial/sf&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vignette for &lt;code&gt;elevatr&lt;/code&gt; package by Jeff Hollister &lt;a href=&#34;https://cran.r-project.org/web/packages/elevatr/vignettes/introduction_to_elevatr.html&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vignettes for the &lt;code&gt;tidycensus&lt;/code&gt; package &lt;a href=&#34;https://github.com/walkerke/tidycensus/tree/master/vignettes&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;USGS introduction to R tutorials &lt;a href=&#34;https://owi.usgs.gov/R/training-curriculum/intro-curriculum/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;USGS R package use tutorials &lt;a href=&#34;https://owi.usgs.gov/R/training-curriculum/usgs-packages/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;USGS R package development tutorial &lt;a href=&#34;https://owi.usgs.gov/R/training-curriculum/r-package-dev/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;USGS other R training &lt;a href=&#34;https://owi.usgs.gov/R/training-curriculum/course-specific-material/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;USGS dataRetrieval tutorial &lt;a href=&#34;https://owi.usgs.gov/R/dataRetrieval.html#1&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SESYNC R tutorials &lt;a href=&#34;https://cyberhelp.sesync.org/lesson/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Software Carpentry introduction to R tutorial &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NEON R tutorials &lt;a href=&#34;https://www.neonscience.org/resources/data-tutorials&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshops-associated-code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Workshops (+ associated code)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GIS in R Workshop at Society for 2018 Freshwater Science Conference by R. Hill &amp;amp; M. Beck &lt;a href=&#34;https://ryan-hill.github.io/sfs-r-gis-2018/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sf&lt;/code&gt; package tutorial with USGS GAGES-II data for NC State BAE 590 Class by S. Saia &lt;a href=&#34;https://github.com/sheilasaia/sf-workshop-bae590&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;blogs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Blogs&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Blog by Jesse Sadler &lt;a href=&#34;https://www.jessesadler.com/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RGeo Blog by Matt Strimas-Mackey &lt;a href=&#34;http://strimas.com/#posts&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rivers &amp;amp; Ranids Blog by Ryan Peek &lt;a href=&#34;https://ryanpeek.github.io/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SESYNC Blog by various &lt;a href=&#34;https://cyberhelp.sesync.org/blog/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;books&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Books&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R for Data Science&lt;/em&gt; by Hadley Wickham &amp;amp; Garrett Grolemund &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Spatial Data Science with R&lt;/em&gt; by various &lt;a href=&#34;https://www.rspatial.org/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Spatial Data Science&lt;/em&gt; by Edzer Pebesma &amp;amp; Roger Bivand &lt;a href=&#34;https://keen-swartz-3146c4.netlify.com/index.html&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Geocomputation with R&lt;/em&gt; by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow &lt;a href=&#34;https://geocompr.robinlovelace.net/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Applied Spatial Data Analysis with R&lt;/em&gt; by Roger Bivand, Edzer Pebesma, and Virgilio Gómez-Rubio &lt;a href=&#34;http://gis.humboldt.edu/OLM/r/Spatial%20Analysis%20With%20R.pdf&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fundamentals of Data Visualization&lt;/em&gt; by Claus O. Wilke &lt;a href=&#34;https://serialmentor.com/dataviz/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Happy Git and GitHub for the useR&lt;/em&gt; by Jenny Bryan &lt;a href=&#34;https://happygitwithr.com/index.html&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Blogdown&lt;/em&gt; by Yihui Xie, Amber Thomas, and Alison Presmanes Hill &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rOpenSci Package Development, Maintenance, and Peer Review&lt;/em&gt; by Brooke Anderson, Scott Chamberlain, Anna Krystalli, Lincoln Mullen, Karthik Ram, Noam Ross, Maëlle Salmon, and Melina Vidoni &lt;a href=&#34;https://ropensci.github.io/dev_guide/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other cool books by RStudio staff &lt;a href=&#34;https://www.rstudio.com/resources/training/books/&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;papers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Papers&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Using R in hydrology: a review of recent developments and future directions&lt;/em&gt; by Slater et al. in review at HESS &lt;a href=&#34;https://www.hydrol-earth-syst-sci-discuss.net/hess-2019-50/&#34;&gt;(link)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;other-help&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Other Help&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rstudio::conf 2018 slides &amp;amp; recorded talks &lt;a href=&#34;https://github.com/simecek/RStudioConf2018Slides&#34;&gt;(link)&lt;/a&gt; or &lt;a href=&#34;https://resources.rstudio.com/rstudio-conf-2018&#34;&gt;(link)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rstudio::conf 2019 slides &amp;amp; recorded talks (&lt;em&gt;coming soon&lt;/em&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Others have created similar summaries of R resources such as &lt;a href=&#34;https://itsalocke.com/blog/r-spatial-resources/&#34;&gt;this post by Steph Locke&lt;/a&gt; and &lt;a href=&#34;https://github.com/rspatialladies/rspatial-resources&#34;&gt;this page by R-Spatial Ladies&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;python-resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Python Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Raoul Collenteur’s GitHub page on Python tools for hydrologists &lt;a href=&#34;https://github.com/raoulcollenteur/Python-Hydrology-Tools&#34;&gt;(link)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you know of any resources that I may have missed (it’s very likely that I missed somehting!) please contact me!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>APIs to the Rescue (&amp; the Census of Agriculture)</title>
      <link>/post/2019-01-04-nass-api/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-01-04-nass-api/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Application program interfaces (APIs) help users access (“API request”) and retrieve (“API response”) data from web-based, data servers via programs like R, Python, etc. If you’re interested in more details, several others before me have done a great job writing about API’s and R: &lt;a href=&#34;https://www.r-bloggers.com/accessing-apis-from-r-and-a-little-r-programming/&#34;&gt;this post by C. Waldhauser&lt;/a&gt; and &lt;a href=&#34;https://tclavelle.github.io/blog/r_and_apis/&#34;&gt;this post by T. Clavelle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I recently learned about a few R packages that help users interface directly with APIs and a few of these are especially interesting for water-minded, data loving people like me. For example the &lt;a href=&#34;https://walkerke.github.io/tidycensus/&#34;&gt;&lt;code&gt;tidycensus&lt;/code&gt; package&lt;/a&gt; developed by &lt;a href=&#34;http://personal.tcu.edu/kylewalker/&#34;&gt;Kyle Walker&lt;/a&gt; allows R users to access the US Census API and download data directly into their R environment. It’s awesome. Additionally, &lt;a href=&#34;https://sheilasaia.rbind.io/post/2018-08-04-usgs-rollify/&#34;&gt;I previously wrote about&lt;/a&gt; the US Geologic Survey and US Environmental Protection Agency’s &lt;a href=&#34;https://cran.r-project.org/web/packages/dataRetrieval/dataRetrieval.pdf&#34;&gt;&lt;code&gt;dataRetrieval&lt;/code&gt; package&lt;/a&gt; that interfaces with the National Water Information System API. Some APIs will require you to have an API key (e.g., the US Census) while others don’t (e.g., the National Water Information System). The key is meant to ensure security on your end as well as on the end of the API administrator. In the case of the US Census, they can keep track of your queries and also make sure that you have permission to access certain types of data, etc. You can easily request an US Census API key &lt;a href=&#34;https://api.census.gov/data/key_signup.html&#34;&gt;here&lt;/a&gt; and read more about the US Census API &lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But what if you’re working with an API that doesn’t already have an R package associated with it? This is the case for the data associated with the US Department of Agriculture National Agricultural Statistics Service (NASS). I could click through the options on NASS’s Quick Stats page &lt;a href=&#34;https://quickstats.nass.usda.gov/&#34;&gt;on the web&lt;/a&gt; and download the data that way; however, I wanted to use R to access the Quick Stats API directly.&lt;/p&gt;
&lt;p&gt;Before jumping into the code, just a brief explainer on the NASS API. The NASS API includes two different types of data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;NASS Agriculture Resource Management Survey (ARMS) - This survey includes data on the “production practices, resource use, and economic well-being of America’s farms and ranches” &lt;a href=&#34;https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/Ag_Resource_Management/&#34;&gt;(NASS ARMS Webiste)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NASS Census of Agriculture - This survey is conducted every 5 years and includes data on the number of US farms and ranches and the people who operate them (as long as more than $1000 was raised from associated agricultural goods). It also includes data on land use, land ownership, production practices, income, and expenses &lt;a href=&#34;https://www.nass.usda.gov/AgCensus/index.php&#34;&gt;(NASS Census of Agriculture Website)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of This Post&lt;/h1&gt;
&lt;p&gt;The main goal of this blog post is to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and plot Agriculture Census data from NASS Quick Stats API using R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Special thanks to &lt;a href=&#34;http://nelson.rbind.io/&#34;&gt;Natalie Nelson&lt;/a&gt; of NC State University and Andrew Dau of NASS for some of the R code that I modified for this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set Up&lt;/h1&gt;
&lt;p&gt;First let’s load the R libraries that we’ll need to run the code in this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;httr&lt;/code&gt; and &lt;code&gt;jsonlite&lt;/code&gt; packages are necessary for interfacing with the Quick Stats API and reformatting API outputs so they can be used in R.&lt;/p&gt;
&lt;p&gt;Let’s use the &lt;code&gt;tidycensus&lt;/code&gt; package to get some county boundaries. This will provide some spatial context for our analysis.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;mapview&lt;/code&gt; packages will help us wrangle and visualize the API outputs.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;purrr&lt;/code&gt; package will help us repetitively apply the same function to each row of the API outputs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If you&amp;#39;ve never used your tidycensus API key in your R session, run this:
census_api_key(&amp;quot;YOUR API KEY GOES HERE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll have to also apply for a NASS API key. We’ll use this later but will definite it here as a string. You can apply for a NASS API key &lt;a href=&#34;https://quickstats.nass.usda.gov/api&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nass_key &amp;lt;- &amp;quot;ADD YOUR NASS API KEY HERE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we’ll define the NASS url and path. In the path you’ll have to specify what type of data you want to query. To specify these you can go to &lt;a href=&#34;https://quickstats.nass.usda.gov/&#34; class=&#34;uri&#34;&gt;https://quickstats.nass.usda.gov/&lt;/a&gt; to see all your commodity options. I haven’t figured out another way to do this but please &lt;a href=&#34;mailto:ssaia@ncsu.edu&#34;&gt;contact me&lt;/a&gt; if you find an alternative.&lt;/p&gt;
&lt;p&gt;For this post I’m selecting the “AG_LAND” commodity, which includes information on the acreage of irrigated farm and ranch lands, because this is the wateR blog after all. Other commodities might include specific crops, etc. I’m also selecting the state of North Carolina (NC) because it seems to always be the subject of spatial mapping &lt;a href=&#34;https://r-spatial.github.io/sf/reference/geos_combine.html&#34;&gt;(e.g., see this post)&lt;/a&gt; in R and is also where I live. ;) You can also leave off the last “&amp;amp;state_alpha=NC” part of the string to get data from all states.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# NASS url
nass_url &amp;lt;- &amp;quot;http://quickstats.nass.usda.gov&amp;quot;

# commodity description of interest
my_commodity_desc &amp;lt;- &amp;quot;AG LAND&amp;quot;

# short description of interest (i.e., &amp;#39;data item&amp;#39; on NASS Quick Stats website)
my_short_desc1 &amp;lt;- &amp;quot;AG LAND, IRRIGATED - ACRES&amp;quot;
my_short_desc2 &amp;lt;- &amp;quot;AG LAND - ACRES&amp;quot;

# query start year
my_year &amp;lt;- &amp;quot;2006&amp;quot;

# state of interest
my_state &amp;lt;- &amp;quot;NC&amp;quot;

# final path string
path_nc_irrig_land &amp;lt;- paste0(&amp;quot;api/api_GET/?key=&amp;quot;, nass_key, &amp;quot;&amp;amp;commodity_desc=&amp;quot;, my_commodity_desc, &amp;quot;&amp;amp;short_desc=&amp;quot;, my_short_desc1, &amp;quot;&amp;amp;short_desc=&amp;quot;, my_short_desc2, &amp;quot;&amp;amp;year__GE=&amp;quot;, my_year, &amp;quot;&amp;amp;state_alpha=&amp;quot;, my_state)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;api-data-query&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;API Data Query&lt;/h1&gt;
&lt;p&gt;Let’s query the NASS API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw_result_nc_irrig_land &amp;lt;- GET(url = nass_url, path = path_nc_irrig_land)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check to see if it worked by looking at &lt;code&gt;status_code&lt;/code&gt;. To read more about the different status codes and their meaning you can visit &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_HTTP_status_codes&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/List_of_HTTP_status_codes&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw_result_nc_irrig_land$status_code&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! We’re wanting to see status code 200 here. It means our query was received and responded to.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reformatting-api-query-output&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reformatting API Query Output&lt;/h1&gt;
&lt;p&gt;If we look at this in your RStudio session it will come in as a ‘Large response’ or in other words as a JSON object. For simplicity sake, we can think of this as a list of lists (i.e., nested lists). We’ll ultimately convert this to a data frame because it’s a little easier to view in R.&lt;/p&gt;
&lt;p&gt;We’ll start unpacking the JSON object using &lt;code&gt;rawToChar()&lt;/code&gt;. We can check the size and look at the first few characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;char_raw_nc_irrig_land &amp;lt;- rawToChar(raw_result_nc_irrig_land$content)

# check size of object
nchar(char_raw_nc_irrig_land)

# view first 50 characthers
substr(char_raw_nc_irrig_land, 1, 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is still a little hard to work with so let’s use &lt;code&gt;fromJSON()&lt;/code&gt; and convert the raw character strings to a large list. Next, we’ll use &lt;code&gt;pmap_dfr&lt;/code&gt; from the &lt;code&gt;purrr&lt;/code&gt; package to loop over each list and bind it by row to make a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_raw_nc_irrig_land &amp;lt;- fromJSON(char_raw_nc_irrig_land)

# view first element (it&amp;#39;s big so we&amp;#39;ll comment it here)
# list_raw_ag_land[[1]]

# apply rbind to each row of the list and convert to a data frame
nc_irrig_land_raw_data &amp;lt;- pmap_dfr(list_raw_nc_irrig_land, rbind)

# look at the data frame
head(nc_irrig_land_raw_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks ok but there are still some things that would be nice to clean up. As mentioned above, we want to focus on the acres of irrigated lands in NC. For simplicity, we’ll just look at farms/ranches with 2,000 ac or more under operation. Let’s step through each line in the piped (i.e., &lt;code&gt;%&amp;gt;%&lt;/code&gt;) code below. See the in-line comments for the details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_irrigated &amp;lt;- nc_irrig_land_raw_data %&amp;gt;%
  
  # filter to select county level irrigation data where farms/ranches with 2,000+ ac operation
  filter(agg_level_desc == &amp;quot;COUNTY&amp;quot;) %&amp;gt;%
  filter(unit_desc == &amp;quot;ACRES&amp;quot;) %&amp;gt;%
  #filter(domaincat_desc == &amp;quot;AREA OPERATED: (2,000 OR MORE ACRES)&amp;quot;) %&amp;gt;%

  # trim white space from ends (note: &amp;#39;Value&amp;#39; is a character here, not a number)
  mutate(value_trim = str_trim(Value)) %&amp;gt;%
  
  # select only the columns we&amp;#39;ll need
  select(state_name, state_alpha, state_ansi, county_code, county_name, asd_desc,
         agg_level_desc, year, prodn_practice_desc_char = prodn_practice_desc, domaincat_desc,
         value_ac_per_yr_char=value_trim, unit_desc) %&amp;gt;%
  
  # filter out entries with codes &amp;#39;(D)&amp;#39; and &amp;#39;(Z)&amp;#39;
  filter(value_ac_per_yr_char != &amp;quot;(D)&amp;quot; &amp;amp; value_ac_per_yr_char != &amp;quot;(Z)&amp;quot;) %&amp;gt;% 
  
  # remove commas from number values and convert to R numeric class
  mutate(value_ac_per_yr = as.numeric(str_remove(value_ac_per_yr_char, &amp;quot;,&amp;quot;))) %&amp;gt;%
  
  # change blanks to underscores in prodn_practice_desc_char for latter processing
  mutate(prodn_practice_desc = str_replace_all(str_to_lower(prodn_practice_desc_char),
                                               &amp;quot;[ ]&amp;quot;, &amp;quot;_&amp;quot;)) %&amp;gt;%
  
  # remove unnecessary columns
  select(-value_ac_per_yr_char, -prodn_practice_desc_char) %&amp;gt;%
  #arrange(county_code, year)
  #irrigated includes 
  
  # we have 2007 and 2012 data and we want irrigated lands and total lands operated
  # (to calculate a percentage of irrigated land) so we use n()&amp;gt;3 to filter out counties
  # that do have both years and info on both irrigated and total lands operated
  group_by(county_code) %&amp;gt;%
  filter(n()&amp;gt;3) %&amp;gt;%
  
  # spread irrigated and total lands operated data and calculate percent irrigated
  group_by(county_code, year) %&amp;gt;%
  spread(prodn_practice_desc, value_ac_per_yr) %&amp;gt;%
  mutate(percent_irrigated = round(irrigated/all_production_practices*100, 1)) %&amp;gt;%
  
  # make a column with the county name and year (we&amp;#39;ll need this for plotting)
  mutate(county_year = paste0(str_to_lower(county_name), &amp;quot;_&amp;quot;, year)) %&amp;gt;%
  
  # make GEOID column to match up with county level spatial data (we&amp;#39;ll need this for mapping)
  mutate(GEOID = paste0(state_ansi, county_code))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the first few rows of the final reformatted NASS data showing the amount of irrigated acres in NC for 2007 and 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(nc_irrigated)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-nass-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting NASS Data&lt;/h1&gt;
&lt;p&gt;Now that we have this nicely formatted data, we can make some figures! Let’s start by making some bar charts comparing the percentage of irrigated land in NC counties where data with available data for 2007 and 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(nc_irrigated) +
  geom_col(aes(x = year, y = percent_irrigated), fill = &amp;quot;grey50&amp;quot;) +
  facet_wrap(~county_name) +
  xlab(&amp;quot;Year&amp;quot;) +
  ylab(&amp;quot;Percent of Total Acres Irrigated (%)&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at this figure, we can see that there was a higher percentage of irrigated land in 2007 for 9 out of 17 (i.e., ~53%) NC counties with available NASS data. We can summarize the total number of acres irrigated for all 17 counties for both 2007 and 2012 to verify this directly using &lt;code&gt;summarize()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_irrigated_summary &amp;lt;- nc_irrigated %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarize(sum_irrigated_ac = sum(irrigated),
            sum_all_production_ac = sum(all_production_practices))

nc_irrigated_summary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that 9,379 more acres were irrigated in 2007 compared to 2012 despite more acres of land being under production in 2012.&lt;/p&gt;
&lt;p&gt;Besides making some bar charts we can also map the irrigation percentages by county. For now, we’ll just filter out the 2007 data for this visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_irrigated_2007 &amp;lt;- nc_irrigated %&amp;gt;%
  filter(year == 2007)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we’ll use the &lt;code&gt;get_acs()&lt;/code&gt; function in the &lt;code&gt;tidycensus&lt;/code&gt; package with &lt;code&gt;geometry = TRUE&lt;/code&gt; to download the &lt;a href=&#34;https://www.census.gov/geo/maps-data/data/tiger.html&#34;&gt;TIGER&lt;/a&gt; county boundaries shape (.shp) file for NC. The variable used here (i.e., “B19013_001”) represents median income but you can use any variable you wish. We’re mostly just interested in the spatial data associated with this and will ignore the tabular (i.e., median income) data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_counties &amp;lt;- get_acs(geography = &amp;quot;county&amp;quot;, state = &amp;quot;NC&amp;quot;, variables = &amp;quot;B19013_001&amp;quot;, 
                       year = 2012, geometry = TRUE, survey = &amp;quot;acs5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second last step is to join &lt;code&gt;nc_irrigated_2007&lt;/code&gt; to the county boundary spatial data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_irrigated_map_2007 &amp;lt;- left_join(nc_counties, nc_irrigated_2007, by = &amp;quot;GEOID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll use &lt;code&gt;mapview()&lt;/code&gt; to make an interactive plot where counties are colored based on the percentage of irrigated land. You can hover your mouse over the counties to see the actual percentages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapviewOptions(vector.palette = colorRampPalette(c(&amp;quot;snow&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;grey10&amp;quot;)))
mapview(nc_irrigated_map_2007, zcol = &amp;quot;percent_irrigated&amp;quot;, legend = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt; Some other thoughts that I wanted to mention before signing off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Querying the NASS API was fairly straightforward, and despite needing to do some considerable data wrangling with the output, the &lt;code&gt;tidyverse&lt;/code&gt; packages (i.e., &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt;) helped a lot. I should note that I spent some time figuring out what the query outputs would look like for different commodities and what aspects of the query outputs I needed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It was interesting to see that a higher percentage of acres were irrigated in NC in 2007 compared to 2012. I can’t say what caused these differences based on these data alone, but it would be interesting to look into whether this finding was linked to the 2007 drought. According to other scientists who lived and researched water resources at the time, &lt;a href=&#34;http://climate.ncsu.edu/climateblog?id=161&#34;&gt;the 2007 drought affected millions of people in NC&lt;/a&gt;. A longer time series of irrigated land might help with this as would overlapping these county level results with drought reports and crop losses.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I can think of a number of other commodities that might be interesting to look at in the NASS Quick Stats data set. I’m assuming that some commonly farmed commodities (i.e., corn) might have more years and locations available. If you’ve used the NASS API for other applications or have any other questions/ideas please let me know!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;updates-since-posting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Updates Since Posting&lt;/h1&gt;
&lt;p&gt;June 2019: Looks like NASS has officially published a &lt;code&gt;usdarnass&lt;/code&gt; package to CRAN! You can read the package documentation &lt;a href=&#34;https://cran.r-project.org/web/packages/usdarnass/usdarnass.pdf&#34;&gt;here&lt;/a&gt;. Looks like there are some nice lookup tables which might be very helpful.&lt;/p&gt;
&lt;p&gt;January 2019: Since first posting this, &lt;a href=&#34;https://twitter.com/julianjon/status/1082849177917317120&#34;&gt;Julian Reyes suggested&lt;/a&gt; using Nicholas Potter’s package called &lt;code&gt;rnassqs&lt;/code&gt; to help automate the NASS API portion of this post. You can read more about the &lt;code&gt;rnassqs&lt;/code&gt; package &lt;a href=&#34;https://github.com/potterzot/rnassqs&#34;&gt;here&lt;/a&gt;. Note to self that I need to check it out!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applied Data Analysis and Mapping in R: Where in Western NC Do Vulnerable Communities and Future Increases in Streamflow Meet?</title>
      <link>/talk/ncgis_mar2019/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>/talk/ncgis_mar2019/</guid>
      <description>

&lt;h2 id=&#34;abstract-br&#34;&gt;Abstract:&lt;/br&gt;&lt;/h2&gt;

&lt;p&gt;Global climate models predict an increase in the frequency and magnitude of storm events in the Southeastern United States over the next several decades. Urban development in this region is expected to double by 2060 and associated increases in impervious surfaces may exacerbate flooding. Communities unable to respond and adapt to future changes in water resources are likely to experience adverse social and economic impacts. Socioeconomic metrics such as the social vulnerability index (SoVI) can be used to predict the location of potentially vulnerable communities but do not incorporate changing biophysical factors such as climate and land use change that impact streamflow. Hydrologic models, such as the Soil and Water Assessment Tool (SWAT), can predict the impacts of future climate and land use on streamflow but do not include socioeconomic factors (e.g., age, transportation access) that may influence a community’s ability to respond and adapt to future streamflow changes. Therefore, there is a need to couple SoVI and SWAT results to identify especially vulnerable communities for climate change adaptation planning. To address this need, we use R to implement a risk matrix framework approach that couples census tract SoVI estimates with changes in SWAT predicted streamflow from 1992-2002 (baseline) and 2050-2070 (global climate model projections) for the Yadkin-Pee Dee (YPD) Watershed in North Carolina, USA. We compare the spatial distribution of subbasins based on (1) SoVI results alone, (2) SWAT results alone, and (3) SoVI and SWAT results combined. SoVI results predicted spatially heterogeneous distributions of social vulnerability throughout the YPD and SWAT results predicted future increases in the number and variability of 10-yr and extreme flow events in southern regions of the YPD. The coupled SoVI and SWAT approach combined biophysical and socioeconomic factors to identify YPD subbasins with vulnerable communities that are likely to experience increases in 10-yr and extreme flow events. This approach can be used as the first of several steps (i.e., community surveying/focus groups, stakeholder visioning, and action taking) associated with effective climate change adaptation planning.&lt;/p&gt;

&lt;p&gt;To read more, visit &lt;a href=&#34;https://ncgisconference.com/&#34; target=&#34;_blank&#34;&gt;the conference website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preprint: An Interdisciplinary Review of Polyphosphate Accumulating Organism (PAO)-Mediated Phosphorus Cycling for Landscape-Scale Water Quality Management</title>
      <link>/publication/pao_review/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/pao_review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coupling Hydrologic Model Outputs and Demographics Data to Inform Climate Change Planning (Poster)</title>
      <link>/talk/agu_dec2018/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 -0500</pubDate>
      
      <guid>/talk/agu_dec2018/</guid>
      <description>

&lt;h2 id=&#34;abstract-br&#34;&gt;Abstract:&lt;/br&gt;&lt;/h2&gt;

&lt;p&gt;General Circulation Models (GCMs) project an increase in the frequency and magnitude of storm events in the Southeastern United States over the next several decades. Additionally, urban development in this region is expected to double by 2060. Communities unable to mitigate or adapt to climate- and land use-change induced impacts on water resources may experience adverse social and economic impacts. Hydrologic model outputs, such as those from the Soil and Water Assessment Tool (SWAT), are helpful at predicting changing hydrologic processes but do not directly incorporate community demographics that are required to assess vulnerability. Therefore, there is a need to couple hydrologic model outputs with demographics data to identify vulnerable communities and support associated climate change adaptation planning efforts. To address these needs, we use a risk matrix framework to couple changes in SWAT simulated streamflow from 1992-2002 (baseline) to 2050-2070 (GCM scenario projections) with census tract social vulnerability index (SoVI) estimates derived from 2010-2014 American Community Survey (ACS) data for the Upper Yadkin-Pee Dee (UYPD) Watershed in North Carolina, USA. We present case studies for select subbasins in the UYPD where especially vulnerable communities overlap with areas projected to experience the highest increases in 10-yr and extreme flows. Compared to using either alone, our results show that coupling SWAT outputs and ACS data provides integrated sociohydrological information that can better inform climate change adaptation planning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://fallmeeting.agu.org/2018/abstract/coupling-hydrologic-model-outputs-and-demographics-data-to-inform-climate-change-planning/&#34; target=&#34;_blank&#34;&gt;Click here to view my AGU abstract. &lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Developing Climate Change Risk Management Tools Using R &amp; Python</title>
      <link>/talk/duke-nov2018/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0400</pubDate>
      
      <guid>/talk/duke-nov2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tidy Tuesday Hydrology Version: Tidying Up SWAT Outputs</title>
      <link>/post/2018-10-16-tidy-swat/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-10-16-tidy-swat/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;I recently participated in Thomas Mock’s &lt;a href=&#34;https://thomasmock.netlify.com/post/tidytuesday-a-weekly-social-data-project-in-r/&#34;&gt;Tidy Tuesday&lt;/a&gt; R challenge with other members of the NC State University R Learning Group. It was a lot of fun and after participating, I felt motivated to share what I’ve been doing to tidy up my own data sets.&lt;/p&gt;
&lt;p&gt;Specifically, I’ve been working with collaborator, &lt;a href=&#34;https://chcs.uncg.edu/about/gis-specialist/&#34;&gt;Kelly Suttles&lt;/a&gt;, to tidy up model outputs from the &lt;a href=&#34;https://swat.tamu.edu/&#34;&gt;Soil and Water Assessment Tool (SWAT)&lt;/a&gt;. SWAT is a hydrologic model used to predict daily and monthly streamflow and water quality at the watershed-scale. It’s pretty widely used and there are even some existing R packages to help researchers calibrate SWAT from the R command line. These include the &lt;a href=&#34;https://cran.r-project.org/web/packages/EcoHydRology/EcoHydRology.pdf&#34;&gt;&lt;code&gt;ecohydRology&lt;/code&gt; package&lt;/a&gt; and the &lt;a href=&#34;https://cran.r-project.org/web/packages/SWATmodel/SWATmodel.pdf&#34;&gt;&lt;code&gt;SWATmodel&lt;/code&gt; package&lt;/a&gt;. Check them out if you work with SWAT and use R.&lt;/p&gt;
&lt;p&gt;I’m sure there are a lot of researchers out there who have developed their own ways of tidying up SWAT outputs but I thought it would be helpful to share some of the functions I’ve recently created to help Kelly migrate her SWAT output analysis from Excel to R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of This Post&lt;/h1&gt;
&lt;p&gt;The goals of this blog post are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discuss how to tidy up SWAT daily and monthly outputs.&lt;/li&gt;
&lt;li&gt;Create functions to make tidying up SWAT outputs easier.&lt;/li&gt;
&lt;li&gt;Share reproducible data analysis workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Special thanks to Kelly Suttles for generating and sharing these SWAT outputs with me.&lt;/p&gt;
&lt;p&gt;If you’re interested in reading more about what Kelly and I have been doing with SWAT outputs, you can read Kelly’s recently paper published in &lt;em&gt;Science of the Total Environment&lt;/em&gt; &lt;a href=&#34;https://www.fs.usda.gov/treesearch/pubs/56780&#34;&gt;here&lt;/a&gt;. I’m currently working on a second paper that relies on Kelly’s SWAT model outputs and I’ll update this post once that’s published.&lt;/p&gt;
&lt;p&gt;Please feel free to use the functions below for your own SWAT output tidying and analysis! Please contact me on &lt;a href=&#34;https://twitter.com/sheilasaia?lang=en&#34;&gt;Twitter&lt;/a&gt; or any other method &lt;a href=&#34;https://sheilasaia.rbind.io/&#34;&gt;here&lt;/a&gt; if you find any mistakes, have suggestions, or know of any other SWAT data analysis resources for R.&lt;/p&gt;
&lt;p&gt;First let’s load the R libraries that we’ll need to run the code in this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;daily-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily Data&lt;/h1&gt;
&lt;p&gt;Next let’s load the daily SWAT outputs first (we’ll work on monthly outputs later). These are generated by SWAT and can be found in the SWAT project &lt;code&gt;Scenarios &amp;gt; Default &amp;gt; TxtInOut&lt;/code&gt; directory. In this post, I’ll just be working with the &lt;code&gt;output.rch&lt;/code&gt; files, which represent the main channel outputs for each subbasin within the larger, SWAT delineated watershed. You can read more about the different SWAT output files &lt;a href=&#34;https://swat.tamu.edu/media/69395/ch32_output.pdf&#34;&gt;here&lt;/a&gt;. In this particular example, I’ll be working with SWAT outputs from the Yadkin-Pee Dee River Watershed (YPD) in North Carolina, USA. The YPD has 28 subbasins as delineated by SWAT.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define data folder paths
daily_data_path &amp;lt;- paste0(here::here(), &amp;quot;/static/data/2018-10-16-tidy-swat/daily/output.rch&amp;quot;)

# load daily data
output_daily_rch_raw &amp;lt;- read_csv(daily_data_path)

head(output_daily_rch_raw)
## # A tibble: 6 x 1
##   `1`                                                                      
##   &amp;lt;chr&amp;gt;                                                                    
## 1 SWAT May 20 2015    VER 2015/Rev 637                                    …
## 2 General Input/Output section (file.cio):                                 
## 3 9/30/2016 12:00:00 AM ARCGIS-SWAT interface AV                           
## 4 RCH      GIS  MO DA   YR     AREAkm2  FLOW_INcms FLOW_OUTcms     EVAPcms…
## 5 REACH    1        0   1  1 1982   0.8080E+03  0.1230E+02  0.1260E+02  0.…
## 6 REACH    2        0   1  1 1982   0.3187E+04  0.1486E+03  0.1538E+03  0.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the basic &lt;code&gt;read_csv()&lt;/code&gt; and without doing any formatting, we notice a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These data are being read in as one big column&lt;/li&gt;
&lt;li&gt;There are several lines of text describing the &lt;code&gt;output.rch&lt;/code&gt; file before the data begins&lt;/li&gt;
&lt;li&gt;Several of the column names have characters that R might not like (e.g., / and a space)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s skip the first 9 columns and set the header argument to false. We’ll manually add the header names later, and when we do, we can reformat the column names using tidy principles (i.e., no / and no spaces). We can also use &lt;code&gt;read_table2()&lt;/code&gt; instead of &lt;code&gt;read_csv()&lt;/code&gt; to deal with the unequal spacing between columns in the text file. More specifically, if you open the &lt;code&gt;output.rch&lt;/code&gt; file in a text editor, you’ll notice that the month column doesn’t always have the same number of spaces before the month number. If you just use &lt;code&gt;read_table()&lt;/code&gt; you’ll loose the first digit of months 10, 11, and 12, which is not good for downstream data analysis. (I know from personal experience.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output_daily_rch_raw &amp;lt;- read_table2(daily_data_path, skip = 9, col_names = FALSE)

head(output_daily_rch_raw, n = 5)
## # A tibble: 5 x 53
##   X1       X2    X3    X4    X5    X6    X7    X8    X9     X10   X11
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 REACH     1     0     1     1  1982  808   12.3  12.6 1.87e-2     0
## 2 REACH     2     0     1     1  1982 3187  149.  154.  6.55e-2     0
## 3 REACH     3     0     1     1  1982 2238  138.  141.  4.33e-2     0
## 4 REACH     4     0     1     1  1982 4365  170.  170.  5.45e-2     0
## 5 REACH     5     0     1     1  1982  625. 107.  107.  1.50e-5     0
## # ... with 42 more variables: X12 &amp;lt;dbl&amp;gt;, X13 &amp;lt;dbl&amp;gt;, X14 &amp;lt;dbl&amp;gt;, X15 &amp;lt;dbl&amp;gt;,
## #   X16 &amp;lt;dbl&amp;gt;, X17 &amp;lt;dbl&amp;gt;, X18 &amp;lt;dbl&amp;gt;, X19 &amp;lt;dbl&amp;gt;, X20 &amp;lt;dbl&amp;gt;, X21 &amp;lt;dbl&amp;gt;,
## #   X22 &amp;lt;dbl&amp;gt;, X23 &amp;lt;dbl&amp;gt;, X24 &amp;lt;dbl&amp;gt;, X25 &amp;lt;dbl&amp;gt;, X26 &amp;lt;dbl&amp;gt;, X27 &amp;lt;dbl&amp;gt;,
## #   X28 &amp;lt;dbl&amp;gt;, X29 &amp;lt;dbl&amp;gt;, X30 &amp;lt;dbl&amp;gt;, X31 &amp;lt;dbl&amp;gt;, X32 &amp;lt;dbl&amp;gt;, X33 &amp;lt;dbl&amp;gt;,
## #   X34 &amp;lt;dbl&amp;gt;, X35 &amp;lt;dbl&amp;gt;, X36 &amp;lt;dbl&amp;gt;, X37 &amp;lt;dbl&amp;gt;, X38 &amp;lt;dbl&amp;gt;, X39 &amp;lt;dbl&amp;gt;,
## #   X40 &amp;lt;dbl&amp;gt;, X41 &amp;lt;dbl&amp;gt;, X42 &amp;lt;dbl&amp;gt;, X43 &amp;lt;dbl&amp;gt;, X44 &amp;lt;dbl&amp;gt;, X45 &amp;lt;dbl&amp;gt;,
## #   X46 &amp;lt;dbl&amp;gt;, X47 &amp;lt;dbl&amp;gt;, X48 &amp;lt;dbl&amp;gt;, X49 &amp;lt;dbl&amp;gt;, X50 &amp;lt;dbl&amp;gt;, X51 &amp;lt;dbl&amp;gt;,
## #   X52 &amp;lt;dbl&amp;gt;, X53 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That looks a lot better but we still need to add header names. You can find additional descriptions on these column names in the &lt;a href=&#34;https://swat.tamu.edu/media/69395/ch32_output.pdf&#34;&gt;SWAT &lt;code&gt;output.rch&lt;/code&gt; documentation&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a new dataframe to save final output
output_daily_rch &amp;lt;- output_daily_rch_raw

# column names list
daily_rch_col_names &amp;lt;- c(&amp;quot;FILE&amp;quot;,&amp;quot;RCH&amp;quot;,&amp;quot;GIS&amp;quot;,&amp;quot;MO&amp;quot;,&amp;quot;DA&amp;quot;,&amp;quot;YR&amp;quot;,&amp;quot;AREAkm2&amp;quot;,&amp;quot;FLOW_INcms&amp;quot;,&amp;quot;FLOW_OUTcms&amp;quot;,&amp;quot;EVAPcms&amp;quot;,&amp;quot;TLOSScms&amp;quot;,&amp;quot;SED_INtons&amp;quot;,&amp;quot;SED_OUTtons&amp;quot;,&amp;quot;SEDCONCmg_kg&amp;quot;,&amp;quot;ORGN_INkg&amp;quot;,&amp;quot;ORGN_OUTkg&amp;quot;,&amp;quot;ORGP_INkg&amp;quot;,&amp;quot;ORGP_OUTkg&amp;quot;,&amp;quot;NO3_INkg&amp;quot;,&amp;quot;NO3_OUTkg&amp;quot;,&amp;quot;NH4_INkg&amp;quot;,&amp;quot;NH4_OUTkg&amp;quot;,&amp;quot;NO2_INkg&amp;quot;,&amp;quot;NO2_OUTkg&amp;quot;,&amp;quot;MINP_INkg&amp;quot;,&amp;quot;MINP_OUTkg&amp;quot;,&amp;quot;CHLA_INkg&amp;quot;,&amp;quot;CHLA_OUTkg&amp;quot;,&amp;quot;CBOD_INkg&amp;quot;,&amp;quot;CBOD_OU/.Tkg&amp;quot;,&amp;quot;DISOX_INkg&amp;quot;,&amp;quot;DISOX_OUTkg&amp;quot;,&amp;quot;SOLPST_INmg&amp;quot;,&amp;quot;SOLPST_OUTmg&amp;quot;,&amp;quot;SORPST_INmg&amp;quot;,&amp;quot;SORPST_OUTmg&amp;quot;,&amp;quot;REACTPSTmg&amp;quot;,&amp;quot;VOLPSTmg&amp;quot;,&amp;quot;SETTLPSTmg&amp;quot;,&amp;quot;RESUSP_PSTmg&amp;quot;,&amp;quot;DIFFUSEPSTmg&amp;quot;,&amp;quot;REACBEDPSTmg&amp;quot;,&amp;quot;BURYPSTmg&amp;quot;,&amp;quot;BED_PSTmg&amp;quot;,&amp;quot;BACTP_OUTct&amp;quot;,&amp;quot;BACTLP_OUTct&amp;quot;,&amp;quot;CMETAL_1kg&amp;quot;,&amp;quot;CMETAL_2kg&amp;quot;,&amp;quot;CMETAL_3kg&amp;quot;,&amp;quot;TOTNkg&amp;quot;,&amp;quot;TOTPkg&amp;quot;,&amp;quot;NO3_mg_l&amp;quot;,&amp;quot;WTMPdegc&amp;quot;)

# reassign column names
colnames(output_daily_rch) &amp;lt;- daily_rch_col_names

head(output_daily_rch, n = 5)
## # A tibble: 5 x 53
##   FILE    RCH   GIS    MO    DA    YR AREAkm2 FLOW_INcms FLOW_OUTcms
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 REACH     1     0     1     1  1982    808        12.3        12.6
## 2 REACH     2     0     1     1  1982   3187       149.        154. 
## 3 REACH     3     0     1     1  1982   2238       138.        141. 
## 4 REACH     4     0     1     1  1982   4365       170.        170. 
## 5 REACH     5     0     1     1  1982    625.      107.        107. 
## # ... with 44 more variables: EVAPcms &amp;lt;dbl&amp;gt;, TLOSScms &amp;lt;dbl&amp;gt;,
## #   SED_INtons &amp;lt;dbl&amp;gt;, SED_OUTtons &amp;lt;dbl&amp;gt;, SEDCONCmg_kg &amp;lt;dbl&amp;gt;,
## #   ORGN_INkg &amp;lt;dbl&amp;gt;, ORGN_OUTkg &amp;lt;dbl&amp;gt;, ORGP_INkg &amp;lt;dbl&amp;gt;, ORGP_OUTkg &amp;lt;dbl&amp;gt;,
## #   NO3_INkg &amp;lt;dbl&amp;gt;, NO3_OUTkg &amp;lt;dbl&amp;gt;, NH4_INkg &amp;lt;dbl&amp;gt;, NH4_OUTkg &amp;lt;dbl&amp;gt;,
## #   NO2_INkg &amp;lt;dbl&amp;gt;, NO2_OUTkg &amp;lt;dbl&amp;gt;, MINP_INkg &amp;lt;dbl&amp;gt;, MINP_OUTkg &amp;lt;dbl&amp;gt;,
## #   CHLA_INkg &amp;lt;dbl&amp;gt;, CHLA_OUTkg &amp;lt;dbl&amp;gt;, CBOD_INkg &amp;lt;dbl&amp;gt;,
## #   `CBOD_OU/.Tkg` &amp;lt;dbl&amp;gt;, DISOX_INkg &amp;lt;dbl&amp;gt;, DISOX_OUTkg &amp;lt;dbl&amp;gt;,
## #   SOLPST_INmg &amp;lt;dbl&amp;gt;, SOLPST_OUTmg &amp;lt;dbl&amp;gt;, SORPST_INmg &amp;lt;dbl&amp;gt;,
## #   SORPST_OUTmg &amp;lt;dbl&amp;gt;, REACTPSTmg &amp;lt;dbl&amp;gt;, VOLPSTmg &amp;lt;dbl&amp;gt;,
## #   SETTLPSTmg &amp;lt;dbl&amp;gt;, RESUSP_PSTmg &amp;lt;dbl&amp;gt;, DIFFUSEPSTmg &amp;lt;dbl&amp;gt;,
## #   REACBEDPSTmg &amp;lt;dbl&amp;gt;, BURYPSTmg &amp;lt;dbl&amp;gt;, BED_PSTmg &amp;lt;dbl&amp;gt;,
## #   BACTP_OUTct &amp;lt;dbl&amp;gt;, BACTLP_OUTct &amp;lt;dbl&amp;gt;, CMETAL_1kg &amp;lt;dbl&amp;gt;,
## #   CMETAL_2kg &amp;lt;dbl&amp;gt;, CMETAL_3kg &amp;lt;dbl&amp;gt;, TOTNkg &amp;lt;dbl&amp;gt;, TOTPkg &amp;lt;dbl&amp;gt;,
## #   NO3_mg_l &amp;lt;dbl&amp;gt;, WTMPdegc &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s so tidy! We can summarize all those steps into one function that we’ll call &lt;code&gt;tidy_daily_rch_file()&lt;/code&gt; that takes the raw input that we’ve read in (using &lt;code&gt;read_table2(&amp;quot;output.rch&amp;quot;, col_names=FALSE, skip=9)&lt;/code&gt;) and outputs a formatted daily SWAT output table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_daily_rch_file &amp;lt;- function(daily_rch_raw_data) {
  # import daily_rch_raw_data into R session using: 
  # daily_rch_raw_data &amp;lt;- read_table2(&amp;quot;output.rch&amp;quot;, col_names=FALSE, skip=9)
  
  # column names
  daily_rch_col_names &amp;lt;- c(&amp;quot;FILE&amp;quot;,&amp;quot;RCH&amp;quot;,&amp;quot;GIS&amp;quot;,&amp;quot;MO&amp;quot;,&amp;quot;DA&amp;quot;,&amp;quot;YR&amp;quot;,&amp;quot;AREAkm2&amp;quot;,&amp;quot;FLOW_INcms&amp;quot;,&amp;quot;FLOW_OUTcms&amp;quot;,&amp;quot;EVAPcms&amp;quot;,&amp;quot;TLOSScms&amp;quot;,&amp;quot;SED_INtons&amp;quot;,&amp;quot;SED_OUTtons&amp;quot;,&amp;quot;SEDCONCmg_kg&amp;quot;,&amp;quot;ORGN_INkg&amp;quot;,&amp;quot;ORGN_OUTkg&amp;quot;,&amp;quot;ORGP_INkg&amp;quot;,&amp;quot;ORGP_OUTkg&amp;quot;,&amp;quot;NO3_INkg&amp;quot;,&amp;quot;NO3_OUTkg&amp;quot;,&amp;quot;NH4_INkg&amp;quot;,&amp;quot;NH4_OUTkg&amp;quot;,&amp;quot;NO2_INkg&amp;quot;,&amp;quot;NO2_OUTkg&amp;quot;,&amp;quot;MINP_INkg&amp;quot;,&amp;quot;MINP_OUTkg&amp;quot;,&amp;quot;CHLA_INkg&amp;quot;,&amp;quot;CHLA_OUTkg&amp;quot;,&amp;quot;CBOD_INkg&amp;quot;,&amp;quot;CBOD_OU/.Tkg&amp;quot;,&amp;quot;DISOX_INkg&amp;quot;,&amp;quot;DISOX_OUTkg&amp;quot;,&amp;quot;SOLPST_INmg&amp;quot;,&amp;quot;SOLPST_OUTmg&amp;quot;,&amp;quot;SORPST_INmg&amp;quot;,&amp;quot;SORPST_OUTmg&amp;quot;,&amp;quot;REACTPSTmg&amp;quot;,&amp;quot;VOLPSTmg&amp;quot;,&amp;quot;SETTLPSTmg&amp;quot;,&amp;quot;RESUSP_PSTmg&amp;quot;,&amp;quot;DIFFUSEPSTmg&amp;quot;,&amp;quot;REACBEDPSTmg&amp;quot;,&amp;quot;BURYPSTmg&amp;quot;,&amp;quot;BED_PSTmg&amp;quot;,&amp;quot;BACTP_OUTct&amp;quot;,&amp;quot;BACTLP_OUTct&amp;quot;,&amp;quot;CMETAL_1kg&amp;quot;,&amp;quot;CMETAL_2kg&amp;quot;,&amp;quot;CMETAL_3kg&amp;quot;,&amp;quot;TOTNkg&amp;quot;,&amp;quot;TOTPkg&amp;quot;,&amp;quot;NO3_mg_l&amp;quot;,&amp;quot;WTMPdegc&amp;quot;)
  
  # reassign column names
  colnames(daily_rch_raw_data) &amp;lt;- daily_rch_col_names
  
  # return output
  return(daily_rch_raw_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now it will just take two lines of code to read in our daily SWAT &lt;code&gt;output.rch&lt;/code&gt; file and tidy it up!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output_daily_rch_raw_test &amp;lt;- read_table2(daily_data_path, skip = 9, col_names = FALSE)

output_daily_rch_test &amp;lt;- tidy_daily_rch_file(output_daily_rch_raw_test)

head(output_daily_rch_test, n = 5)
## # A tibble: 5 x 53
##   FILE    RCH   GIS    MO    DA    YR AREAkm2 FLOW_INcms FLOW_OUTcms
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 REACH     1     0     1     1  1982    808        12.3        12.6
## 2 REACH     2     0     1     1  1982   3187       149.        154. 
## 3 REACH     3     0     1     1  1982   2238       138.        141. 
## 4 REACH     4     0     1     1  1982   4365       170.        170. 
## 5 REACH     5     0     1     1  1982    625.      107.        107. 
## # ... with 44 more variables: EVAPcms &amp;lt;dbl&amp;gt;, TLOSScms &amp;lt;dbl&amp;gt;,
## #   SED_INtons &amp;lt;dbl&amp;gt;, SED_OUTtons &amp;lt;dbl&amp;gt;, SEDCONCmg_kg &amp;lt;dbl&amp;gt;,
## #   ORGN_INkg &amp;lt;dbl&amp;gt;, ORGN_OUTkg &amp;lt;dbl&amp;gt;, ORGP_INkg &amp;lt;dbl&amp;gt;, ORGP_OUTkg &amp;lt;dbl&amp;gt;,
## #   NO3_INkg &amp;lt;dbl&amp;gt;, NO3_OUTkg &amp;lt;dbl&amp;gt;, NH4_INkg &amp;lt;dbl&amp;gt;, NH4_OUTkg &amp;lt;dbl&amp;gt;,
## #   NO2_INkg &amp;lt;dbl&amp;gt;, NO2_OUTkg &amp;lt;dbl&amp;gt;, MINP_INkg &amp;lt;dbl&amp;gt;, MINP_OUTkg &amp;lt;dbl&amp;gt;,
## #   CHLA_INkg &amp;lt;dbl&amp;gt;, CHLA_OUTkg &amp;lt;dbl&amp;gt;, CBOD_INkg &amp;lt;dbl&amp;gt;,
## #   `CBOD_OU/.Tkg` &amp;lt;dbl&amp;gt;, DISOX_INkg &amp;lt;dbl&amp;gt;, DISOX_OUTkg &amp;lt;dbl&amp;gt;,
## #   SOLPST_INmg &amp;lt;dbl&amp;gt;, SOLPST_OUTmg &amp;lt;dbl&amp;gt;, SORPST_INmg &amp;lt;dbl&amp;gt;,
## #   SORPST_OUTmg &amp;lt;dbl&amp;gt;, REACTPSTmg &amp;lt;dbl&amp;gt;, VOLPSTmg &amp;lt;dbl&amp;gt;,
## #   SETTLPSTmg &amp;lt;dbl&amp;gt;, RESUSP_PSTmg &amp;lt;dbl&amp;gt;, DIFFUSEPSTmg &amp;lt;dbl&amp;gt;,
## #   REACBEDPSTmg &amp;lt;dbl&amp;gt;, BURYPSTmg &amp;lt;dbl&amp;gt;, BED_PSTmg &amp;lt;dbl&amp;gt;,
## #   BACTP_OUTct &amp;lt;dbl&amp;gt;, BACTLP_OUTct &amp;lt;dbl&amp;gt;, CMETAL_1kg &amp;lt;dbl&amp;gt;,
## #   CMETAL_2kg &amp;lt;dbl&amp;gt;, CMETAL_3kg &amp;lt;dbl&amp;gt;, TOTNkg &amp;lt;dbl&amp;gt;, TOTPkg &amp;lt;dbl&amp;gt;,
## #   NO3_mg_l &amp;lt;dbl&amp;gt;, WTMPdegc &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now can use &lt;code&gt;output_daily_rch_test&lt;/code&gt; for some downstream data analysis. Let’s first plot the range in monthly discharge (as predicted by SWAT) just at the outlet (i.e., subbasin number 28) versus the simulation year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select outlet data
outlet_daily_data &amp;lt;- output_daily_rch_test %&amp;gt;% 
  filter(RCH == 28)

# plot
ggplot(data = outlet_daily_data, mapping = aes(x = as.factor(YR), y = FLOW_OUTcms)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.1, width = 0.1) +
  xlab(&amp;quot;Year&amp;quot;) +
  ylab(&amp;quot;Daily Discharge (cms)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-16-tidy-swat_files/figure-html/plotting%20daily%20data%20part%201-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, let’s plot the hydrograph of discharge at the the outlet (i.e., subbasin number 28) for 1982.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select outlet data for 1982
outlet_daily_1982_data &amp;lt;- output_daily_rch_test %&amp;gt;% 
  filter(RCH == 28) %&amp;gt;%
  filter(YR == 1982) %&amp;gt;%
  mutate(date = as.Date(paste0(YR,&amp;quot;-&amp;quot;,MO,&amp;quot;-&amp;quot;,DA)))

ggplot(data = outlet_daily_1982_data, mapping = aes(x = date, y = FLOW_OUTcms)) +
  geom_line(color = &amp;quot;blue&amp;quot;) +
  xlab(&amp;quot;Date&amp;quot;) +
  ylab(&amp;quot;Daily Discharge (cms)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-16-tidy-swat_files/figure-html/plotting%20daily%20data%20part%202-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Monthly Data&lt;/h1&gt;
&lt;p&gt;You can also run SWAT at a monthly time-step. This might happen if you are trying to predict sediment or nutrient loads when you only have these measurements at the monthly time-step. The SWAT &lt;code&gt;output.rch&lt;/code&gt; file is slightly different if you use the monthly time-step. More on this later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define data folder paths
monthly_data_path &amp;lt;- paste0(here::here(), &amp;quot;/static/data/2018-10-16-tidy-swat/monthly/output.rch&amp;quot;)

# load daily data
output_monthly_rch_raw &amp;lt;- read_csv(monthly_data_path)

head(output_monthly_rch_raw)
## # A tibble: 6 x 1
##   `1`                                                                      
##   &amp;lt;chr&amp;gt;                                                                    
## 1 SWAT May 20 2015    VER 2015/Rev 637                                    …
## 2 General Input/Output section (file.cio):                                 
## 3 9/1/2017 12:00:00 AM ARCGIS-SWAT interface AV                            
## 4 RCH      GIS   MON     AREAkm2  FLOW_INcms FLOW_OUTcms     EVAPcms    TL…
## 5 REACH    1        0     1  0.8080E+03  0.3818E+01  0.3911E+01  0.1456E-0…
## 6 REACH    2        0     1  0.3187E+04  0.1997E+02  0.2023E+02  0.5534E-0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the basic &lt;code&gt;read_csv()&lt;/code&gt; and without doing any formatting, we notice a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These data are being read in as one big column (Like the daily data!)&lt;/li&gt;
&lt;li&gt;There are several lines of text describing the &lt;code&gt;output.rch&lt;/code&gt; file before the data begins (Like the daily data!)&lt;/li&gt;
&lt;li&gt;Several of the column names have characters that R might not like (e.g., / and a space) (Like the daily data!)&lt;/li&gt;
&lt;li&gt;The year of the simulation is included in the MO (i.e., month) and represents a summary of the data for that year (This is new to the month file.)&lt;/li&gt;
&lt;li&gt;There is no YR column (This is new to the month file.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we use the same formatting we used before with the daily data and skip down to the 336 row, the fourth point above becomes more clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read in monthly data
output_monthly_rch_raw &amp;lt;- read_table2(monthly_data_path, skip = 9, col_names = FALSE)

# column names
montly_rch_col_names=c(&amp;quot;FILE&amp;quot;,&amp;quot;RCH&amp;quot;,&amp;quot;GIS&amp;quot;,&amp;quot;MO&amp;quot;,&amp;quot;AREAkm2&amp;quot;,&amp;quot;FLOW_INcms&amp;quot;,&amp;quot;FLOW_OUTcms&amp;quot;,&amp;quot;EVAPcms&amp;quot;,&amp;quot;TLOSScms&amp;quot;,&amp;quot;SED_INtons&amp;quot;,&amp;quot;SED_OUTtons&amp;quot;,&amp;quot;SEDCONCmg_kg&amp;quot;,&amp;quot;ORGN_INkg&amp;quot;,&amp;quot;ORGN_OUTkg&amp;quot;,&amp;quot;ORGP_INkg&amp;quot;,&amp;quot;ORGP_OUTkg&amp;quot;,&amp;quot;NO3_INkg&amp;quot;,&amp;quot;NO3_OUTkg&amp;quot;,&amp;quot;NH4_INkg&amp;quot;,&amp;quot;NH4_OUTkg&amp;quot;,&amp;quot;NO2_INkg&amp;quot;,&amp;quot;NO2_OUTkg&amp;quot;,&amp;quot;MINP_INkg&amp;quot;,&amp;quot;MINP_OUTkg&amp;quot;,&amp;quot;CHLA_INkg&amp;quot;,&amp;quot;CHLA_OUTkg&amp;quot;,&amp;quot;CBOD_INkg&amp;quot;,&amp;quot;CBOD_OUTkg&amp;quot;,&amp;quot;DISOX_INkg&amp;quot;,&amp;quot;DISOX_OUTkg&amp;quot;,&amp;quot;SOLPST_INmg&amp;quot;,&amp;quot;SOLPST_OUTmg&amp;quot;,&amp;quot;SORPST_INmg&amp;quot;,&amp;quot;SORPST_OUTmg&amp;quot;,&amp;quot;REACTPSTmg&amp;quot;,&amp;quot;VOLPSTmg&amp;quot;,&amp;quot;SETTLPSTmg&amp;quot;,&amp;quot;RESUSP_PSTmg&amp;quot;,&amp;quot;DIFFUSEPSTmg&amp;quot;,&amp;quot;REACBEDPSTmg&amp;quot;,&amp;quot;BURYPSTmg&amp;quot;,&amp;quot;BED_PSTmg&amp;quot;,&amp;quot;BACTP_OUTct&amp;quot;,&amp;quot;BACTLP_OUTct&amp;quot;,&amp;quot;CMETALnumkg&amp;quot;,&amp;quot;CMETALnum2kg&amp;quot;,&amp;quot;CMETALnum3kg&amp;quot;,&amp;quot;TOTNkg&amp;quot;,&amp;quot;TOTPkg&amp;quot;,&amp;quot;NO3ConcMg_l&amp;quot;,&amp;quot;WTMPdegc&amp;quot;)

# reassign column names
colnames(output_monthly_rch_raw) = montly_rch_col_names

output_monthly_rch_raw[336:338,]
## # A tibble: 3 x 51
##   FILE    RCH   GIS    MO AREAkm2 FLOW_INcms FLOW_OUTcms EVAPcms TLOSScms
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 REACH    28     0    12   17780     137.        137.    0.0587     5.66
## 2 REACH     1     0  1982     808       8.47        8.39  0.0658     3.51
## 3 REACH     2     0  1982    3187      42.0        41.7   0.231     12.3 
## # ... with 42 more variables: SED_INtons &amp;lt;dbl&amp;gt;, SED_OUTtons &amp;lt;dbl&amp;gt;,
## #   SEDCONCmg_kg &amp;lt;dbl&amp;gt;, ORGN_INkg &amp;lt;dbl&amp;gt;, ORGN_OUTkg &amp;lt;dbl&amp;gt;,
## #   ORGP_INkg &amp;lt;dbl&amp;gt;, ORGP_OUTkg &amp;lt;dbl&amp;gt;, NO3_INkg &amp;lt;dbl&amp;gt;, NO3_OUTkg &amp;lt;dbl&amp;gt;,
## #   NH4_INkg &amp;lt;dbl&amp;gt;, NH4_OUTkg &amp;lt;dbl&amp;gt;, NO2_INkg &amp;lt;dbl&amp;gt;, NO2_OUTkg &amp;lt;dbl&amp;gt;,
## #   MINP_INkg &amp;lt;dbl&amp;gt;, MINP_OUTkg &amp;lt;dbl&amp;gt;, CHLA_INkg &amp;lt;dbl&amp;gt;, CHLA_OUTkg &amp;lt;dbl&amp;gt;,
## #   CBOD_INkg &amp;lt;dbl&amp;gt;, CBOD_OUTkg &amp;lt;dbl&amp;gt;, DISOX_INkg &amp;lt;dbl&amp;gt;,
## #   DISOX_OUTkg &amp;lt;dbl&amp;gt;, SOLPST_INmg &amp;lt;dbl&amp;gt;, SOLPST_OUTmg &amp;lt;dbl&amp;gt;,
## #   SORPST_INmg &amp;lt;dbl&amp;gt;, SORPST_OUTmg &amp;lt;dbl&amp;gt;, REACTPSTmg &amp;lt;dbl&amp;gt;,
## #   VOLPSTmg &amp;lt;dbl&amp;gt;, SETTLPSTmg &amp;lt;dbl&amp;gt;, RESUSP_PSTmg &amp;lt;dbl&amp;gt;,
## #   DIFFUSEPSTmg &amp;lt;dbl&amp;gt;, REACBEDPSTmg &amp;lt;dbl&amp;gt;, BURYPSTmg &amp;lt;dbl&amp;gt;,
## #   BED_PSTmg &amp;lt;dbl&amp;gt;, BACTP_OUTct &amp;lt;dbl&amp;gt;, BACTLP_OUTct &amp;lt;dbl&amp;gt;,
## #   CMETALnumkg &amp;lt;dbl&amp;gt;, CMETALnum2kg &amp;lt;dbl&amp;gt;, CMETALnum3kg &amp;lt;dbl&amp;gt;,
## #   TOTNkg &amp;lt;dbl&amp;gt;, TOTPkg &amp;lt;dbl&amp;gt;, NO3ConcMg_l &amp;lt;dbl&amp;gt;, WTMPdegc &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the month column goes from 12 to 1982; 1982 is the first year of the simulation.&lt;/p&gt;
&lt;p&gt;Overall, the monthly SWAT output in combining &lt;strong&gt;two types of data&lt;/strong&gt; (i.e., month number and year number) in one column. Likewise, the data associated with the corresponding rows of these yearly summary columns is lumped together with the monthly SWAT outputs. This is definitely not &lt;em&gt;tidy&lt;/em&gt;; tidy data consists of unique variables in each column and unique observations in each row. The easiest way tidy this up is to identify and delete the yearly summary rows. If you want, you can save them for later QA/QC. That is, you can compare the yearly average summaries interspersed within the monthly SWAT output to yearly averages that you manually calculate from the monthly SWAT outputs. In this post, we’ll delete the yearly summary rows.&lt;/p&gt;
&lt;p&gt;We’ll keep track of the simulation years, take out the summary rows, and add a column called &lt;code&gt;YR&lt;/code&gt; that we’ll fill in the next step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# record the simulation years for later
simulation_years &amp;lt;- unique(output_monthly_rch_raw$MO)[unique(output_monthly_rch_raw$MO) &amp;gt; 12]
num_years &amp;lt;- length(simulation_years)
    
# remove summary rows but make YR column to hold these data              
output_monthly_rch &amp;lt;- output_monthly_rch_raw %&amp;gt;%
  mutate(notes = ifelse(MO &amp;gt; 12, &amp;quot;summary&amp;quot;, &amp;quot;not_summary&amp;quot;)) %&amp;gt;% # note whether summary or not
  mutate(YR = as.numeric(&amp;quot;NA&amp;quot;)) %&amp;gt;% # make an empty column to fill later
  filter(notes == &amp;quot;not_summary&amp;quot;) %&amp;gt;% # remove annual summary for each subbasin
  select(-notes) %&amp;gt;% # remove it to keep it tidy
  select(FILE:MO, YR, AREAkm2:WTMPdegc) # rearrange columns

dim(output_monthly_rch_raw)
## [1] 1456   51
dim(output_monthly_rch)
## [1] 1344   52&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see &lt;code&gt;output_monthly_rch_raw&lt;/code&gt; had 1456 rows and the &lt;code&gt;output_monthly_rch&lt;/code&gt; now has 1344. That’s 112 less rows, which checks out because 4 years times 28 subbasins is 112. That is, there was one summary row for each subbasin for each year that we just deleted.&lt;/p&gt;
&lt;p&gt;Now let’s loop through the data and fill in the year column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define some parameters we&amp;#39;ll need
num_subs &amp;lt;- max(unique(output_monthly_rch$RCH))
num_months &amp;lt;- 12
num_per_month &amp;lt;- num_subs * num_months

# for loop
str=1
end=num_per_month
for (i in 1:length(simulation_years)) {
  output_monthly_rch$YR[str:end] = rep(simulation_years[i], num_per_month)
  str=end+1
  end=str+num_per_month-1
}

head(output_monthly_rch, n = 5)
## # A tibble: 5 x 52
##   FILE    RCH   GIS    MO    YR AREAkm2 FLOW_INcms FLOW_OUTcms EVAPcms
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 REACH     1     0     1  1982    808        3.82        3.91 1.46e-2
## 2 REACH     2     0     1  1982   3187       20.0        20.2  5.53e-2
## 3 REACH     3     0     1  1982   2238       15.5        15.9  4.35e-2
## 4 REACH     4     0     1  1982   4365       25.5        25.4  8.00e-2
## 5 REACH     5     0     1  1982    625.       8.99        8.99 3.74e-5
## # ... with 43 more variables: TLOSScms &amp;lt;dbl&amp;gt;, SED_INtons &amp;lt;dbl&amp;gt;,
## #   SED_OUTtons &amp;lt;dbl&amp;gt;, SEDCONCmg_kg &amp;lt;dbl&amp;gt;, ORGN_INkg &amp;lt;dbl&amp;gt;,
## #   ORGN_OUTkg &amp;lt;dbl&amp;gt;, ORGP_INkg &amp;lt;dbl&amp;gt;, ORGP_OUTkg &amp;lt;dbl&amp;gt;, NO3_INkg &amp;lt;dbl&amp;gt;,
## #   NO3_OUTkg &amp;lt;dbl&amp;gt;, NH4_INkg &amp;lt;dbl&amp;gt;, NH4_OUTkg &amp;lt;dbl&amp;gt;, NO2_INkg &amp;lt;dbl&amp;gt;,
## #   NO2_OUTkg &amp;lt;dbl&amp;gt;, MINP_INkg &amp;lt;dbl&amp;gt;, MINP_OUTkg &amp;lt;dbl&amp;gt;, CHLA_INkg &amp;lt;dbl&amp;gt;,
## #   CHLA_OUTkg &amp;lt;dbl&amp;gt;, CBOD_INkg &amp;lt;dbl&amp;gt;, CBOD_OUTkg &amp;lt;dbl&amp;gt;, DISOX_INkg &amp;lt;dbl&amp;gt;,
## #   DISOX_OUTkg &amp;lt;dbl&amp;gt;, SOLPST_INmg &amp;lt;dbl&amp;gt;, SOLPST_OUTmg &amp;lt;dbl&amp;gt;,
## #   SORPST_INmg &amp;lt;dbl&amp;gt;, SORPST_OUTmg &amp;lt;dbl&amp;gt;, REACTPSTmg &amp;lt;dbl&amp;gt;,
## #   VOLPSTmg &amp;lt;dbl&amp;gt;, SETTLPSTmg &amp;lt;dbl&amp;gt;, RESUSP_PSTmg &amp;lt;dbl&amp;gt;,
## #   DIFFUSEPSTmg &amp;lt;dbl&amp;gt;, REACBEDPSTmg &amp;lt;dbl&amp;gt;, BURYPSTmg &amp;lt;dbl&amp;gt;,
## #   BED_PSTmg &amp;lt;dbl&amp;gt;, BACTP_OUTct &amp;lt;dbl&amp;gt;, BACTLP_OUTct &amp;lt;dbl&amp;gt;,
## #   CMETALnumkg &amp;lt;dbl&amp;gt;, CMETALnum2kg &amp;lt;dbl&amp;gt;, CMETALnum3kg &amp;lt;dbl&amp;gt;,
## #   TOTNkg &amp;lt;dbl&amp;gt;, TOTPkg &amp;lt;dbl&amp;gt;, NO3ConcMg_l &amp;lt;dbl&amp;gt;, WTMPdegc &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s way more tidy! We can summarize all those steps into one function that called &lt;code&gt;tidy_monthly_rch_file()&lt;/code&gt;. This function takes the raw input that we’ve read into our R session (using &lt;code&gt;read_table2(&amp;quot;output.rch&amp;quot;, col_names=FALSE, skip=9)&lt;/code&gt;) and outputs a formatted monthly SWAT output table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_monthly_rch_file &amp;lt;- function(monthly_rch_raw_data) {
  # import monthly_rch_raw_data into R session using: 
  # monthly_rch_raw_data &amp;lt;- read_table2(&amp;quot;output.rch&amp;quot;, col_names=FALSE, skip=9)
  
  # column names
  montly_rch_col_names=c(&amp;quot;FILE&amp;quot;,&amp;quot;RCH&amp;quot;,&amp;quot;GIS&amp;quot;,&amp;quot;MO&amp;quot;,&amp;quot;AREAkm2&amp;quot;,&amp;quot;FLOW_INcms&amp;quot;,&amp;quot;FLOW_OUTcms&amp;quot;,&amp;quot;EVAPcms&amp;quot;,&amp;quot;TLOSScms&amp;quot;,&amp;quot;SED_INtons&amp;quot;,&amp;quot;SED_OUTtons&amp;quot;,&amp;quot;SEDCONCmg_kg&amp;quot;,&amp;quot;ORGN_INkg&amp;quot;,&amp;quot;ORGN_OUTkg&amp;quot;,&amp;quot;ORGP_INkg&amp;quot;,&amp;quot;ORGP_OUTkg&amp;quot;,&amp;quot;NO3_INkg&amp;quot;,&amp;quot;NO3_OUTkg&amp;quot;,&amp;quot;NH4_INkg&amp;quot;,&amp;quot;NH4_OUTkg&amp;quot;,&amp;quot;NO2_INkg&amp;quot;,&amp;quot;NO2_OUTkg&amp;quot;,&amp;quot;MINP_INkg&amp;quot;,&amp;quot;MINP_OUTkg&amp;quot;,&amp;quot;CHLA_INkg&amp;quot;,&amp;quot;CHLA_OUTkg&amp;quot;,&amp;quot;CBOD_INkg&amp;quot;,&amp;quot;CBOD_OUTkg&amp;quot;,&amp;quot;DISOX_INkg&amp;quot;,&amp;quot;DISOX_OUTkg&amp;quot;,&amp;quot;SOLPST_INmg&amp;quot;,&amp;quot;SOLPST_OUTmg&amp;quot;,&amp;quot;SORPST_INmg&amp;quot;,&amp;quot;SORPST_OUTmg&amp;quot;,&amp;quot;REACTPSTmg&amp;quot;,&amp;quot;VOLPSTmg&amp;quot;,&amp;quot;SETTLPSTmg&amp;quot;,&amp;quot;RESUSP_PSTmg&amp;quot;,&amp;quot;DIFFUSEPSTmg&amp;quot;,&amp;quot;REACBEDPSTmg&amp;quot;,&amp;quot;BURYPSTmg&amp;quot;,&amp;quot;BED_PSTmg&amp;quot;,&amp;quot;BACTP_OUTct&amp;quot;,&amp;quot;BACTLP_OUTct&amp;quot;,&amp;quot;CMETALnumkg&amp;quot;,&amp;quot;CMETALnum2kg&amp;quot;,&amp;quot;CMETALnum3kg&amp;quot;,&amp;quot;TOTNkg&amp;quot;,&amp;quot;TOTPkg&amp;quot;,&amp;quot;NO3ConcMg_l&amp;quot;,&amp;quot;WTMPdegc&amp;quot;)
  
  # reassign column names
  colnames(monthly_rch_raw_data) = montly_rch_col_names
  
  # dataset parameters
  simulation_years &amp;lt;- unique(monthly_rch_raw_data$MO)[unique(monthly_rch_raw_data$MO) &amp;gt; 12]
  num_years &amp;lt;- length(simulation_years)
  num_subs &amp;lt;- max(unique(monthly_rch_raw_data$RCH))
  num_months &amp;lt;- 12
  num_per_month &amp;lt;- num_subs * num_months
  
  # remove summary rows from raw data              
  monthly_rch_raw_data_temp &amp;lt;- monthly_rch_raw_data %&amp;gt;%
    mutate(notes = ifelse(MO &amp;gt; 12, &amp;quot;summary&amp;quot;, &amp;quot;not_summary&amp;quot;)) %&amp;gt;% # note whether summary or not
    mutate(YR = ifelse(MO &amp;gt; 12, MO,as.numeric(&amp;quot;NA&amp;quot;))) %&amp;gt;% # make an empty column to fill later
    filter(notes == &amp;quot;not_summary&amp;quot;) %&amp;gt;% # remove annual summary for each subbasin
    select(-notes) %&amp;gt;% # remove it to keep it tidy
    select(FILE:MO, YR, AREAkm2:WTMPdegc) # rearrange columns
  
  # for loop to set year
  str=1
  end=num_per_month
  for (i in 1:length(simulation_years)) {
    monthly_rch_raw_data_temp$YR[str:end] = rep(simulation_years[i], num_per_month)
    str=end+1
    end=str+num_per_month-1
  }
  
  # return output
  return(monthly_rch_raw_data_temp)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now it will just take two lines of code to read in our monthly SWAT &lt;code&gt;output.rch&lt;/code&gt; file and tidy it up!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output_monthly_rch_raw_test &amp;lt;- read_table2(monthly_data_path, skip = 9, col_names = FALSE)

output_monthly_rch_test &amp;lt;- tidy_monthly_rch_file(output_monthly_rch_raw_test)

head(output_monthly_rch_test, n = 5)
## # A tibble: 5 x 52
##   FILE    RCH   GIS    MO    YR AREAkm2 FLOW_INcms FLOW_OUTcms EVAPcms
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 REACH     1     0     1  1982    808        3.82        3.91 1.46e-2
## 2 REACH     2     0     1  1982   3187       20.0        20.2  5.53e-2
## 3 REACH     3     0     1  1982   2238       15.5        15.9  4.35e-2
## 4 REACH     4     0     1  1982   4365       25.5        25.4  8.00e-2
## 5 REACH     5     0     1  1982    625.       8.99        8.99 3.74e-5
## # ... with 43 more variables: TLOSScms &amp;lt;dbl&amp;gt;, SED_INtons &amp;lt;dbl&amp;gt;,
## #   SED_OUTtons &amp;lt;dbl&amp;gt;, SEDCONCmg_kg &amp;lt;dbl&amp;gt;, ORGN_INkg &amp;lt;dbl&amp;gt;,
## #   ORGN_OUTkg &amp;lt;dbl&amp;gt;, ORGP_INkg &amp;lt;dbl&amp;gt;, ORGP_OUTkg &amp;lt;dbl&amp;gt;, NO3_INkg &amp;lt;dbl&amp;gt;,
## #   NO3_OUTkg &amp;lt;dbl&amp;gt;, NH4_INkg &amp;lt;dbl&amp;gt;, NH4_OUTkg &amp;lt;dbl&amp;gt;, NO2_INkg &amp;lt;dbl&amp;gt;,
## #   NO2_OUTkg &amp;lt;dbl&amp;gt;, MINP_INkg &amp;lt;dbl&amp;gt;, MINP_OUTkg &amp;lt;dbl&amp;gt;, CHLA_INkg &amp;lt;dbl&amp;gt;,
## #   CHLA_OUTkg &amp;lt;dbl&amp;gt;, CBOD_INkg &amp;lt;dbl&amp;gt;, CBOD_OUTkg &amp;lt;dbl&amp;gt;, DISOX_INkg &amp;lt;dbl&amp;gt;,
## #   DISOX_OUTkg &amp;lt;dbl&amp;gt;, SOLPST_INmg &amp;lt;dbl&amp;gt;, SOLPST_OUTmg &amp;lt;dbl&amp;gt;,
## #   SORPST_INmg &amp;lt;dbl&amp;gt;, SORPST_OUTmg &amp;lt;dbl&amp;gt;, REACTPSTmg &amp;lt;dbl&amp;gt;,
## #   VOLPSTmg &amp;lt;dbl&amp;gt;, SETTLPSTmg &amp;lt;dbl&amp;gt;, RESUSP_PSTmg &amp;lt;dbl&amp;gt;,
## #   DIFFUSEPSTmg &amp;lt;dbl&amp;gt;, REACBEDPSTmg &amp;lt;dbl&amp;gt;, BURYPSTmg &amp;lt;dbl&amp;gt;,
## #   BED_PSTmg &amp;lt;dbl&amp;gt;, BACTP_OUTct &amp;lt;dbl&amp;gt;, BACTLP_OUTct &amp;lt;dbl&amp;gt;,
## #   CMETALnumkg &amp;lt;dbl&amp;gt;, CMETALnum2kg &amp;lt;dbl&amp;gt;, CMETALnum3kg &amp;lt;dbl&amp;gt;,
## #   TOTNkg &amp;lt;dbl&amp;gt;, TOTPkg &amp;lt;dbl&amp;gt;, NO3ConcMg_l &amp;lt;dbl&amp;gt;, WTMPdegc &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now can use &lt;code&gt;output_monthly_rch_test&lt;/code&gt; for some downstream data analysis. Let’s first plot the range in SWAT predicted monthly discharge just at the outlet (i.e., subbasin number 28) versus the simulation year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select outlet data
outlet_monthly_data &amp;lt;- output_monthly_rch_test %&amp;gt;% 
  filter(RCH == 28)

# plot
ggplot(data = outlet_monthly_data, mapping = aes(x = as.factor(YR), y = FLOW_OUTcms)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.25, width = 0.1) +
  xlab(&amp;quot;Year&amp;quot;) +
  ylab(&amp;quot;Monthly Discharge (cms)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-16-tidy-swat_files/figure-html/plotting%20monthly%20data%20part%201-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, let’s plot the monthly discharge all subbasins versus simulation year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = output_monthly_rch_test,
       mapping = aes(x = as.factor(YR), y = FLOW_OUTcms, color = as.factor(RCH))) +
  geom_jitter(alpha = 0.25, width = 0.1) +
  facet_wrap(~RCH, ncol = 7) +
  xlab(&amp;quot;Year&amp;quot;) +
  ylab(&amp;quot;Monthly Discharge (cms)&amp;quot;) +
  scale_color_discrete(name = &amp;quot;Subbasin ID&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-16-tidy-swat_files/figure-html/plotting%20monthly%20data%20part%202-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Last, we’ll plot the range of monthly discharge for all subbasins versus month.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = output_monthly_rch_test, mapping = aes(x = as.factor(MO), y = FLOW_OUTcms)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.25, width = 0.1) +
  xlab(&amp;quot;Month&amp;quot;) +
  ylab(&amp;quot;Monthly Discharge (cms)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-16-tidy-swat_files/figure-html/plotting%20monthly%20data%20part%203-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some other thoughts that I wanted to mention before signing off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I think using R to tidy up SWAT outputs is a great way to introduce principles of reproducible to your hydrology data analysis workflows. You can do this by saving the functions above in scripts. If you want to take it a step further you can save the functions to their own R script; where the name of the script is the same as the function name. For example, the code for the &lt;code&gt;tidy_daily_rch_file()&lt;/code&gt; function should be saved to &lt;code&gt;tidy_daily_rch_file.R&lt;/code&gt;. You can then load this file into your workspace using &lt;code&gt;load(&amp;quot;...tidy_daily_rch_file.R&amp;quot;)&lt;/code&gt; and use it after loading.&lt;/li&gt;
&lt;li&gt;When tidying up the monthly SWAT outputs, I used a for loop but I’m sure there are other more efficient ways of doing this. Maybe using &lt;code&gt;tidyr::fill()&lt;/code&gt; or &lt;code&gt;purrr::map()&lt;/code&gt;? I’ll keep working on this.&lt;/li&gt;
&lt;li&gt;Above, I mentioned saving the yearly summary data in a separate data frame and running QA/QC on this with the newly tidied monthly data. This is something I’d like to do eventually.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Please feel free to use the functions above for your own SWAT output tidying and analysis! Please contact me on &lt;a href=&#34;https://twitter.com/sheilasaia?lang=en&#34;&gt;Twitter&lt;/a&gt; or any other method &lt;a href=&#34;https://sheilasaia.rbind.io/&#34;&gt;here&lt;/a&gt; if you find any mistakes, have suggestions, or know of any other SWAT data analysis resources for R.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applying Climate Change Risk Management Tools to Combine Social Vulnerability and Future Streamflow Projections</title>
      <link>/talk/fer-oct2018/</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/talk/fer-oct2018/</guid>
      <description>

&lt;h2 id=&#34;abstract-br&#34;&gt;Abstract:&lt;/br&gt;&lt;/h2&gt;

&lt;p&gt;General Circulation Models (GCMs) project an increase in the frequency and magnitude of storm events in the Southeastern United States over the next several decades. Additionally, urban development in this region is expected to double by 2060. Communities unable to mitigate and adapt to climate- and land use-change induced impacts on water resources may experience adverse social and economic impacts. Hydrologic model outputs, such as those from the Soil and Water Assessment Tool (SWAT), are helpful at predicting changing hydrologic processes but do not directly incorporate community demographics that are required to assess vulnerability. Therefore, there is a need to couple hydrologic model outputs with socioeconomic data to identify vulnerable communities and support associated climate change adaptation planning efforts. To address these needs, we use a risk matrix framework to couple changes in SWAT simulated streamflow from 1992-2002 (baseline) to 2050-2070 (GCM scenario projections) with census tract social vulnerability index (SoVI) estimates derived from 2010-2014 American Community Survey (ACS) data for the Upper Yadkin-Pee Dee (UYPD) Watershed in North Carolina, USA. To evaluate our results, we compare the spatial distribution of subbasins in urgent need of climate change adaptation planning based on three approaches using: SWAT results only, SoVI results only, and the spatial intersection of SWAT and SoVI results using the risk matrix framework. 10-yr and extreme events increased and were more variable between the baseline and projection SWAT streamflow datasets with especially large increases in the middle and lower parts of the YPD. Socially vulnerable communities were heterogeneously distributed throughout the YPD. The spatial intersection of SWAT and SoVI results integrated biophysical and socioeconomic factors and can be used to inform the first of several steps (i.e., community surveying, stakeholder visioning, and action taking) associated with effective climate change adaptation planning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using tibbletime::rollify with USGS Streamgage Data</title>
      <link>/post/2018-08-04-usgs-rollify/</link>
      <pubDate>Wed, 05 Sep 2018 21:12:00 -0500</pubDate>
      
      <guid>/post/2018-08-04-usgs-rollify/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;While attending rstudio::conf 2018, I heard about the &lt;a href=&#34;https://cran.r-project.org/web/packages/tibbletime/tibbletime.pdf&#34;&gt;tibbletime package&lt;/a&gt; developed by Davis Vaughan and Matt Dancho for analysis of time series data. In his &lt;a href=&#34;https://www.rstudio.com/resources/videos/the-future-of-time-series-and-financial-analysis-in-the-tidyverse/&#34;&gt;conference talk&lt;/a&gt;, Davis Vaughan presented several business/finance examples to showcase &lt;code&gt;tibbletime&lt;/code&gt;’s functionality and mentioned a few, general non-business applications at the end of his talk. I couldn’t help but think about how this package might be especially helpful for environmental scientists working with time series data. I really liked &lt;code&gt;rollify()&lt;/code&gt;, which could be used to create and apply custom moving-window functions (e.g., &lt;code&gt;mean()&lt;/code&gt;) to time series data; where the size of the window (e.g., 3 days) could also be specified. I was aware of the &lt;code&gt;movingAve()&lt;/code&gt; and &lt;code&gt;movingDiff()&lt;/code&gt; functions in the &lt;a href=&#34;https://pubs.usgs.gov/of/2015/1202/downloads/appendix1.pdf&#34;&gt;smwrBase package&lt;/a&gt; developed by the U.S. Geological Survey (USGS), which calculate moving average and difference, respectively. However, &lt;code&gt;rollify()&lt;/code&gt; has more flexibility; it can be used to take a rolling standard deviation, a rolling ratio of two build in R functions, or a rolling custom function. The possibilities are endless.&lt;/p&gt;
&lt;p&gt;Shortly after the conference, I also heard about the &lt;a href=&#34;https://cran.r-project.org/web/packages/dataRetrieval/dataRetrieval.pdf&#34;&gt;dataRetrieval package&lt;/a&gt; developed by the USGS from my friend, Dr. Kristina Hopkins. This package allows users to import USGS streamflow data (and any other data associated with &lt;a href=&#34;https://water.usgs.gov/nsip/definition9.html&#34;&gt;USGS streamgages&lt;/a&gt;) directly into R. Very exciting!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals-of-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goals of this Post&lt;/h1&gt;
&lt;p&gt;This all sounded so amazing and I had to try it out, which brings me to the goals of this blog post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;dataRetrieval&lt;/code&gt; package to download streamflow data into R for two USGS streamgages&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;tibbletime&lt;/code&gt; package (along with other &lt;code&gt;tidyverse&lt;/code&gt; packages) to aggregate and plot these data for different time periods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Special thanks to &lt;a href=&#34;https://twitter.com/kghopkin?lang=en&#34;&gt;Dr. Kristina Hopkins&lt;/a&gt; at the USGS in Raleigh, NC for sharing her &lt;code&gt;dataRetrieval&lt;/code&gt; code and to &lt;a href=&#34;https://www.usgs.gov/staff-profiles/laura-decicco?qt-staff_profile_science_products=3#qt-staff_profile_science_products&#34;&gt;Dr. Laura DeCicco&lt;/a&gt; at the USGS in Middleton, WI for pointing out key features of the &lt;code&gt;dataRetrieval&lt;/code&gt; package!&lt;/p&gt;
&lt;p&gt;First let’s load the R libraries that we’ll need to run the code in this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(tibbletime)
library(dataRetrieval)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading data, you’ll have to specify a desired date range and look up the USGS streamgage identification number(s) for streamgage(s) of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define start and end dates
start_date &amp;lt;- &amp;quot;2017-01-01&amp;quot;
end_date &amp;lt;- &amp;quot;2018-01-01&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll also need to specify the parameter code for the data you want to download (e.g., discharge, stage height, etc.). But how do we know what the parameter code is for our favorite data? The USFS have included a look-up table for that! To figure this out, we can save &lt;code&gt;parameterCdFile&lt;/code&gt; to a variable named &lt;code&gt;parameter_code_metadata&lt;/code&gt; and &lt;code&gt;View(parameter_code_metadata)&lt;/code&gt; it to see the parameter code look-up table in RStudio. We can even search through different columns of these data by clicking on the ‘Filter’ funnel icon in the RStudio &lt;code&gt;View()&lt;/code&gt; window.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look for parameters you want
parameter_code_metadata &amp;lt;- parameterCdFile

# after some looking around, we see that we want code &amp;quot;00060&amp;quot; for discharge in cubic feet per second (cfs)
my_parameter_code &amp;lt;- &amp;quot;00060&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After looking through the table, I figured out that 00060 is the code for discharge in cubic feet per second (cfs). We can save this to the variable &lt;code&gt;my_parameter_code&lt;/code&gt;, which leaves one last step…&lt;/p&gt;
&lt;p&gt;We have to decide which streamgage to choose! If you’d rather do this manually/look at a map, you can you can go to the USGS &lt;a href=&#34;https://waterdata.usgs.gov/nwis/inventory&#34;&gt;Water of the Nation website&lt;/a&gt; and use their mapping tool to look up sites visually. Alternatively, you can use the look-up tables in the &lt;code&gt;dataRetrieval&lt;/code&gt; package to do this in R.&lt;/p&gt;
&lt;p&gt;Let’s say we want to see all the sites in North Carolina. We can save the output from the &lt;code&gt;whatNWISsites()&lt;/code&gt; function to the variable &lt;code&gt;nc_sites&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nc_sites &amp;lt;- whatNWISsites(stateCd = &amp;quot;NC&amp;quot;, parameterCD = &amp;quot;00060&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like we did for the parameter code, we can use ‘Filter’ funnel icon in the RStudio &lt;code&gt;View()&lt;/code&gt; window to look for sites that might be interesting to us. I looked for ‘Eno River’ sites and discovered two that I’ll use here: Eno River at Hillsborough, NC (#02085000) and Eno River at Durham, NC (#02085070). The Hillsborough site is upstream of Durham and is surrounded by less urban development.&lt;/p&gt;
&lt;p&gt;Note: If you want to search what data is available (e.g., period of record, available parameters, etc.) you can use &lt;code&gt;whatNWISdata()&lt;/code&gt;. See &lt;code&gt;?whatNWISdata&lt;/code&gt; for help on how to use this. For ease of use (and because Dr. Kristina Hopkins shared them with me! Thank you!), the service codes include: “dv” for daily data, “iv” for instantaneuos data, “qw” for water quality data, “sv” for site visit data, “pk” for peak measurement data, and “gw” for groundwater data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define sites (you can also just use one site here)
site_numbers &amp;lt;- c(&amp;quot;02085070&amp;quot;, &amp;quot;02085000&amp;quot;)

# save site info to a variable so we can look at it later
site_info &amp;lt;- readNWISsite(site_numbers)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to download discharge data for these two sites!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discharge_raw &amp;lt;- readNWISuv(site_numbers, 
                            my_parameter_code, 
                            tz=&amp;#39;America/Jamaica&amp;#39;, 
                            start_date, end_date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s tidy thse data up a bit. We’ll change some of the column names so they make more sense later to us, add a column with the site name, and select only the most important columns. If you are using this for something more formal, you will want to do some QA/QC by checking out the discharge_code column. You can read more about what these codes mean &lt;a href=&#34;https://help.waterdata.usgs.gov/output-formats&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discharge = discharge_raw %&amp;gt;%
  select(site_number = site_no, 
         date_time = dateTime,
         discharge_cfs = X_00060_00000,
         discharge_code = X_00060_00000_cd,
         time_zone = tz_cd) %&amp;gt;%
  mutate(site_name = case_when(
    site_number == &amp;quot;02085000&amp;quot; ~ &amp;quot;eno_rv_hillsb&amp;quot;,
    site_number == &amp;quot;02085070&amp;quot; ~ &amp;quot;eno_rv_durham&amp;quot;)) %&amp;gt;%
  select(site_name, date_time, discharge_cfs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;head()&lt;/code&gt;, we see that we’ve downloaded 15-min intervals of streamflow data for both sites.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(discharge, n = 10)
##        site_name           date_time discharge_cfs
## 1  eno_rv_hillsb 2017-01-01 00:00:00          9.68
## 2  eno_rv_hillsb 2017-01-01 00:15:00          9.68
## 3  eno_rv_hillsb 2017-01-01 00:30:00          9.68
## 4  eno_rv_hillsb 2017-01-01 00:45:00          9.68
## 5  eno_rv_hillsb 2017-01-01 01:00:00          9.68
## 6  eno_rv_hillsb 2017-01-01 01:15:00          9.68
## 7  eno_rv_hillsb 2017-01-01 01:30:00          9.32
## 8  eno_rv_hillsb 2017-01-01 01:45:00          9.32
## 9  eno_rv_hillsb 2017-01-01 02:00:00          9.32
## 10 eno_rv_hillsb 2017-01-01 02:15:00          9.32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say we need daily discharge data to calibrate a hydrology model. We can aggregate the 15-min data to daily data pretty easily using &lt;code&gt;tidyverse&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discharge_daily &amp;lt;- discharge %&amp;gt;%
  mutate(year = year(date_time), 
         month = month(date_time),
         day = day(date_time)) %&amp;gt;%
  mutate(date = ymd(paste0(year, &amp;quot;-&amp;quot;, month, &amp;quot;-&amp;quot;, day))) %&amp;gt;%
  select(site_name, date, discharge_cfs) %&amp;gt;%
  group_by(site_name, date) %&amp;gt;%
  summarize(avg_daily_discharge_cfs = mean(discharge_cfs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try plotting the daily hydrograph (i.e., average daily discharge vs time) for both sites.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = discharge_daily) +
  geom_line(aes(x = date, y = avg_daily_discharge_cfs, color = site_name)) +
  facet_wrap(~site_name, nrow = 2, ncol = 1) +
  ylab(&amp;quot;Average Daily Discharge (cfs)&amp;quot;) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = &amp;quot;black&amp;quot;),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-04-usgs-rollify_files/figure-html/plotting%20daily%20data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, we see that the two sites mimic one another over this one year time period. Remember, the Eno River at Hillsborough site is upstream of the Eno River at Durham site. We also see that the Eno River at Durham site (top, red line) has some higher peaks than the Eno River at Hillborough site (bottom, teal line). This also makes sense since the Durham site is surrounded by more urbanized land than the Hillsborough site.&lt;/p&gt;
&lt;p&gt;Ok, so let’s get back to the &lt;code&gt;tibbletime&lt;/code&gt; package. What if we want to compare different time spans? That is, not just daily, but maybe hourly or half day. First we have to define our rolling functions and their associated time spans.&lt;/p&gt;
&lt;p&gt;For this post, let’s create half hour, hourly, half day, and day rolls.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_roll_30min &amp;lt;- rollify(mean, window = 2)
mean_roll_1hr &amp;lt;- rollify(mean, window = 4)
mean_roll_12hr &amp;lt;- rollify(mean, window = 48)
mean_roll_1d &amp;lt;- rollify(mean, window = 96)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this post we’ll focus on using the mean in our rolling functions but I can think of several other functions that might be useful for streamgage data lovers: difference, standard deviation, or maybe even the ratio between two calculations. You could even write your own function and roll that. Pretty flexible, right?&lt;/p&gt;
&lt;p&gt;After you you write the function &lt;code&gt;my_roll_function &amp;lt;- rollify(&amp;lt;function&amp;gt;, window = &amp;lt;row span&amp;gt;)&lt;/code&gt; then you can call &lt;code&gt;my_roll_function(&amp;lt;original data column&amp;gt;)&lt;/code&gt; in &lt;code&gt;mutate()&lt;/code&gt; to create a new column. Depending on the function you’re using, you could maybe even do a nested roll based on a previous column you created. This seems helpful if you’re taking differences or summations across different time scales.&lt;/p&gt;
&lt;p&gt;Let’s roll (our data)! :D&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discharge_roll &amp;lt;- discharge %&amp;gt;%
  group_by(site_name) %&amp;gt;%
  mutate(
    halfhr_avg_discharge_cfs = mean_roll_30min(discharge_cfs),
    hourly_avg_discharge_cfs = mean_roll_1hr(discharge_cfs),
    twelvehr_avg_discharge_cfs = mean_roll_12hr(discharge_cfs), 
    daily_avg_discharge_cfs = mean_roll_1d(discharge_cfs)) %&amp;gt;%
  na.omit()

# use na.omit() to delete some of the first cells in the dataframe that are NA&amp;#39;s due to rolling&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at these data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(discharge_roll, n = 10) 
## # A tibble: 10 x 7
## # Groups:   site_name [1]
##    site_name date_time           discharge_cfs halfhr_avg_disc…
##    &amp;lt;chr&amp;gt;     &amp;lt;dttm&amp;gt;                      &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 eno_rv_h… 2017-01-01 23:45:00          12.1             12.1
##  2 eno_rv_h… 2017-01-02 00:00:00          12.1             12.1
##  3 eno_rv_h… 2017-01-02 00:15:00          11.6             11.8
##  4 eno_rv_h… 2017-01-02 00:30:00          11.6             11.6
##  5 eno_rv_h… 2017-01-02 00:45:00          11.6             11.6
##  6 eno_rv_h… 2017-01-02 01:00:00          12.1             11.8
##  7 eno_rv_h… 2017-01-02 01:15:00          12.1             12.1
##  8 eno_rv_h… 2017-01-02 01:30:00          12.1             12.1
##  9 eno_rv_h… 2017-01-02 01:45:00          12.1             12.1
## 10 eno_rv_h… 2017-01-02 02:00:00          12.5             12.3
## # ... with 3 more variables: hourly_avg_discharge_cfs &amp;lt;dbl&amp;gt;,
## #   twelvehr_avg_discharge_cfs &amp;lt;dbl&amp;gt;, daily_avg_discharge_cfs &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataframe has maintained it’s length (except for the NA columns at the top we deleted using &lt;code&gt;na.omit()&lt;/code&gt;) and there are four new columns with aggregations of discharge at the time scales we specified in our rolling functions.&lt;/p&gt;
&lt;p&gt;We can plot this as a hydrograph as we did above or we can also plot each aggregation period against the original 15-min data to see how they compare. Let’s show the latter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_colors &amp;lt;- c(&amp;quot;a_half_hour&amp;quot; = &amp;quot;grey20&amp;quot;, &amp;quot;b_hour&amp;quot; = &amp;quot;grey40&amp;quot;, &amp;quot;c_half_day&amp;quot; = &amp;quot;grey60&amp;quot;, &amp;quot;d_day&amp;quot; = &amp;quot;grey80&amp;quot;)

ggplot(data = discharge_roll) +
  geom_point(aes(x = discharge_cfs,y = halfhr_avg_discharge_cfs, color = &amp;quot;a_half_hour&amp;quot;), size = 1) +
  geom_point(aes(x = discharge_cfs, y = hourly_avg_discharge_cfs, color = &amp;quot;b_hour&amp;quot;), size = 1) +
  geom_point(aes(x = discharge_cfs, y = twelvehr_avg_discharge_cfs, color = &amp;quot;c_half_day&amp;quot;), size = 1) +
  geom_point(aes(x = discharge_cfs, y = daily_avg_discharge_cfs, color = &amp;quot;d_day&amp;quot;), size = 1) +
  facet_wrap(~ site_name, ncol = 1, nrow = 2) +
  scale_color_manual(name = &amp;quot;Time Scale&amp;quot;, values = my_colors) +
  xlab(&amp;quot;15-min Dicharge (cfs)&amp;quot;) +
  ylab(&amp;quot;Aggregated Discharge (cfs)&amp;quot;) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = &amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-04-usgs-rollify_files/figure-html/plot%20results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are some interesting patterns that (I think) are worth pointing out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Based on the daily hydrograph we plotted above, the Eno River at Durham (top) data set has higher peak events. This was expected given the daily hydrograph we plotted above. We see that discharge values go up to over 10,000 cfs at the Eno River at Durham site whereas for the Eno River at Hillsborough they only go up to about 5,000 cfs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I thought it was interesting that the half hour and hourly data sets match the 15-min time scale closely; we see the points follow the 1:1 line for both. However, the half day and daily data show dampened responses with lower peaks and skew away from the 1:1 line. This dampening wasn’t completely surprising to me but it made me wonder how this would impact downstream applications of daily streamflow data. Is this dampening represented by a linear or non-linear relationship? Is this relationship a function of watershed properties or near gage properties. Lot’s of questions like this come to mind. Most of which deal with being able to account for and maybe even predict this dampening.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The last thing that I wanted to point out was the spiraling nature of these figures. The more we aggregate these data, the more it seems to spiral into the origin at zero. Also, it’s interesting to compare the overlap of time scale spirals (i.e., half day and daily) between the two gages. More specifically, we see that one ring of the half day and daily spirals overlap for the Eno River at Hillsborough site but we don’t see this overlap at the Eno River at Durham site. I wonder why? Is it because urban development has caused a little more disconnection between water stored in the landscape for the Eno River at Durham site? Do antecedent moisture conditions play a larger role in future stream response in the less developed Eno River at Hillsborough site?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are still so many questions floating around in my head but figuring out how to integrate the &lt;code&gt;tibbletime&lt;/code&gt; and &lt;code&gt;dataRetrieval&lt;/code&gt; packages has been fun! If you know of any publications that address these questions or just want to say &lt;code&gt;Hi!&lt;/code&gt;, please let me know &lt;a href=&#34;https://twitter.com/sheilasaia?lang=en&#34;&gt;via Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>/post/2018-07-20-my-first-post/</link>
      <pubDate>Mon, 23 Jul 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-07-20-my-first-post/</guid>
      <description>


&lt;p&gt;Welcome to the wateR blog! I’ve been considering starting a water-centric data science blog for a while now. Getting this blogdown website up and running has helped solidify my motivation to write regularly about water-related data science advances that are available to R users…and maybe Python users too (both languages are so useful!).&lt;/p&gt;
&lt;p&gt;Some of the specific goals of this blog are to (in no special order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Practice developing my science communication and R skills.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Review and demo new packages, data sets, and methods that might be useful to water resources research.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Highlight newly published studies that use data science to advance or could be applied to water resources research.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Discuss the present and future of water data science.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Interview experts about water-related topics.&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Water Quality in a Suburban Watershed</title>
      <link>/project/mrgp-project/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 -0400</pubDate>
      
      <guid>/project/mrgp-project/</guid>
      <description>

&lt;h1 id=&#34;collaborators&#34;&gt;Collaborators&lt;/h1&gt;

&lt;p&gt;R Christie (Mianus River Gorge Preserve), C Nagy (Mianus River Gorge Preserve), MT Walter (Cornell University), PJ Sullivan (Cornell University)&lt;/p&gt;

&lt;h1 id=&#34;timeline&#34;&gt;Timeline&lt;/h1&gt;

&lt;p&gt;2012-2015&lt;/p&gt;

&lt;h1 id=&#34;project-goal&#34;&gt;Project Goal&lt;/h1&gt;

&lt;p&gt;The goal of this project was to test two spatial statistics approaches (i.e., kriging based on Euclidian or stream-wise distances) for predicting in-stream waterquality at unsampled locations in the Mianus River near Bedford, NY and Stanford, CT.&lt;/p&gt;

&lt;h1 id=&#34;project-links&#34;&gt;Project Links&lt;/h1&gt;

&lt;p&gt;Results of this project are currently unpublished.&lt;/p&gt;

&lt;h1 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h1&gt;

&lt;p&gt;Special thanks to the &lt;a href=&#34;http://www.mianus.org/research-and-education/graduate-level/meet-our-raps/&#34; target=&#34;_blank&#34;&gt;Mianus River Gorge Preserve Research Award Program&lt;/a&gt; for supporting this work.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
